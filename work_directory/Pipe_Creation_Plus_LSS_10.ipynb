{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f659186f",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737eb72",
   "metadata": {},
   "source": [
    "This notebook updates Pipe_Creation_Plus_LSS_9.ipynb\n",
    "\n",
    "(Based on Illustris_Zhang_Method/batch_jobs/batch_scripts/make_pipes_3.py code).\n",
    "\n",
    "- It builds upon the Zhang+20 method of creating DMs from the TNG simulation.\n",
    "- It improves upon this by correcting ElectronAbundance in star-forming regions using the Pakmor+18 method allowing them to be used. To do this it calculates the warm-phase gas mass fraction W (=1-X, where X is from Marinacci+17)\n",
    "- It identifies LSS using the Artale+21 method.\n",
    "- It inserts code developed to load snapshots partwise to handle larger simulations\n",
    "- It saves output data to file as it is created, which allows it to be stopped and resumed rather than being started from scratch.\n",
    "- It removes unecessary tests and plots and incorporates all code into a single cell, which can be copied into a .py file to be run as a batch job.\n",
    "- It stores Particle ID and location information which can be used to calculate halo impact factors along tthe line of sight.\n",
    "- It contains an option for multiprocessing different pipes on multiple cpus simultaneously to speed up pipe creation\n",
    "- It cleans up Pipe_Creation_Plus_LSS_6.ipynb, removing much of the testing.\n",
    "- It utilises matchlists which are now stored in /ptmp/ and saved in chunks to allow larger simulations (e.g. TNG300-1) to be processed\n",
    "- It adds 'speed' option for creating subhalo IDs which allows us to test the speed of the code without it. It stores placeholder values of 1 for every subhalo ID. The true values would have to be calculated later. The temporary files created in ProcessSimChunk() and subsequently loaded in the main code have been hard coded with 'SpeedTest' suffixes and the pipes output to a /SpeedTest/ subdirectory to accomodate testing purposes while generating pipes with older code. This should be cleaned up in final version.\n",
    "\n",
    "Notes: \n",
    "\n",
    "- The lines of code which save pipes have been commented out in this version of the code in order not to overwrite existing file\n",
    "- When running on a Jupyter notebook, it seems to begin failing after requesting around 6-7+ cpus simultaneously. This may be an issue with the interactive nodes. Hopeefully, in a script we should be able to use many more (50-70)\n",
    "\n",
    "ELECTRON DENSITY OF SFR CODE ARCHIVE\n",
    "\n",
    "raven:/u/cwalker/Illustris_FRB_Project/oct2_2021_output/IGM_new_scripts/job_raven.py, raven:/u/cwalker/Illustris_FRB_Project/charlie_TNG_lib/charlie_TNG_tools.py\n",
    "PAPERS: Marinacci+17: https://arxiv.org/abs/1610.01594, Pakmor+18: https://arxiv.org/abs/1807.02113\n",
    "\n",
    "PIPE CREATION CODE ARCHIVE\n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/Pipe_Creation_Test.ipynb, raven:/u/cwalker/Illustris_Zhang_Method/Pipe_Creation_Plus_LSS.ipynb, raven:/u/cwalker/Illustris_Zhang_Method/Pipe_Creation_Plus_LSS.ipynb\n",
    "PAPERS: Zhang+20: https://arxiv.org/abs/2011.14494\n",
    "\n",
    "LSS CLASSIFICATION CODE ARCHIVE\n",
    "\n",
    "raven:/u/cwalker/Illustris_FRB_Project/git_Illustris_uploads/mass_fraction_plots/artale_test_121121/SCRIPT_Cel_Auto.py, raven:/u/cwalker/Illustris_FRB_Project/yt-artale-constants.ipynb\n",
    "PAPERS: Artale+21: https://arxiv.org/abs/2102.01092\n",
    "\n",
    "PARTWISE SIMULATION LOADING ARCHIVE\n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/Test_Subset_Loading.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415902a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f4cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes\n",
    "#This is a script for use as a batch job\n",
    "#It is an upgade of make_pipes_3.py.\n",
    "#It incorporates updated code for impact factor analysis\n",
    "#It incorporates code which allows subhalo/particle ID matchlists to be loaded in chunks when stored in /ptmp/\n",
    "#and multiprocessing from ../Illustris_Zhang_Method/Pipe_Creation_Plus_LSS_10.ipynb\n",
    "#but it includes (and hard codes) in a 'speed' option for subhalo ID analyssis which stores placeholder values of 1\n",
    "#instead of calculating them. This is to test how much slowdown is due to the loops in this part of the code. Any temporary files created are hardcoded with the suffix '_SpeedTest' and resulting pipes are redirected to the subdirectory /u/cwalker/Illustris_Zhang_Method/SpeedTest/ so they do not get mixed up with originals during testing while simultaneously creating data for the same sim and snapshot with older code versions.\n",
    "\n",
    "#imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as m\n",
    "import illustris_python as il\n",
    "\n",
    "from contextlib import closing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from numpy import random as rand\n",
    "from astropy import constants as c\n",
    "from charlie_TNG_tools import temp2u\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import binned_statistic_dd\n",
    "from astropy.cosmology import Planck15 as cosmosource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4ae3c",
   "metadata": {},
   "source": [
    "# Parse command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c30db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line inputs: sim=TNG300-1, pipes=5125, snap=99, cpus=1\n"
     ]
    }
   ],
   "source": [
    "#parse command line arguments#\n",
    "\n",
    "#cla = sys.argv                       #comment out reading command line\n",
    "cla = ['','TNG300-1','5125','99','1'] #fake command line input\n",
    "\n",
    "\n",
    "n_inputs = 4\n",
    "\n",
    "if len(cla)!=n_inputs+1:\n",
    "    print(\"Error! {0} arguments required. {1} arguments provided.\".format(n_inputs,len(cla)))\n",
    "    print(\"Exiting script.\")\n",
    "    sys.exit()\n",
    "\n",
    "else:\n",
    "    sim_to_use = str(cla[1])# the simulation to use, e.g. 'TNG50-4'\n",
    "    pipes_per_snap = int(cla[2]) #the number of pipes to create per snapshot, e.g. 5125\n",
    "    snap_to_process = int(cla[3]) #the snapshot number to process, e.g. 99\n",
    "    cpus_to_use = int(cla[4]) #the number of simultaneous cores to load data with.\n",
    "    \n",
    "    print('Command line inputs: sim={0}, pipes={1}, snap={2}, cpus={3}'.format(sim_to_use,\n",
    "                                                                               pipes_per_snap,\n",
    "                                                                               snap_to_process,\n",
    "                                                                               cpus_to_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2bbfd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def gadgetDens2SI(dens):\n",
    "    \"\"\"\n",
    "    Original Artale function to convert TNG densities to SI units\n",
    "\n",
    "    INPUTSRETURNS:\n",
    "\n",
    "    dens : [values] densities from TNG\n",
    "\n",
    "    RETURNS:\n",
    "\n",
    "    dens converted to SI units\n",
    "    \"\"\"\n",
    "    return dens*1E10*cel_Msol_si/cel_hubble/(cel_kpc_si/cel_hubble)**3\n",
    "\n",
    "def TNG_Dens2SI(dens):\n",
    "    \"\"\"\n",
    "    Like gadgetDens2SI but using astropy values for constants\n",
    "    Strips result of units\n",
    "    Developed in raven:/u/cwalker/Illustris_FRB_Project/yt-artale-constants.ipynb\n",
    "\n",
    "\n",
    "    INPUTSRETURNS:\n",
    "\n",
    "    dens : [values] densities from TNG\n",
    "\n",
    "    RETURNS:\n",
    "\n",
    "    dens converted to SI units\n",
    "    \"\"\"\n",
    "    return dens*1E10*c.M_sun.to('kg').value/cosmosource.h/(c.kpc.to('m').value/cosmosource.h)**3\n",
    "\n",
    "def TNG_Dens2SI_astropy(dens):\n",
    "    \"\"\"\n",
    "    Like TNG_Dens2SI but does not strip result of units.\n",
    "    Developed in raven:/u/cwalker/Illustris_FRB_Project/yt-artale-constants.ipynb\n",
    "\n",
    "\n",
    "    INPUTSRETURNS:\n",
    "\n",
    "    dens : [values] densities from TNG\n",
    "\n",
    "    RETURNS:\n",
    "\n",
    "    dens converted to SI units\n",
    "    \"\"\"\n",
    "\n",
    "    return dens*1E10*c.M_sun.to('kg')/cosmosource.h/(c.kpc.to('m')/cosmosource.h)**3\n",
    "\n",
    "\n",
    "def pSplitRange(indrange, numProcs, curProc, inclusive=False):\n",
    "    \"\"\" Divide work for embarassingly parallel problems. \n",
    "    Accept a 2-tuple of [start,end] indices and return a new range subset.\n",
    "    If inclusive==True, then assume the range subset will be used e.g. as input to snapshotSubseet(),\n",
    "    which unlike numpy convention is inclusive in the indices.\"\"\"\n",
    "    assert len(indrange) == 2 and indrange[1] > indrange[0]\n",
    "\n",
    "    if numProcs == 1:\n",
    "        if curProc != 0:\n",
    "            raise Exception(\"Only a single processor but requested curProc>0.\")\n",
    "        return indrange\n",
    "\n",
    "    # split array into numProcs segments, and return the curProc'th segment\n",
    "    splitSize = int(np.floor( (indrange[1]-indrange[0]) / numProcs ))\n",
    "    start = indrange[0] + curProc*splitSize\n",
    "    end   = indrange[0] + (curProc+1)*splitSize\n",
    "\n",
    "    # for last split, make sure it takes any leftovers\n",
    "    if curProc == numProcs-1:\n",
    "        end = indrange[1]\n",
    "\n",
    "    if inclusive and curProc < numProcs-1:\n",
    "        # not for last split/final index, because this should be e.g. NumPart[0]-1 already\n",
    "        end -= 1\n",
    "\n",
    "    return [start,end]\n",
    "\n",
    "\n",
    "def loadSubset(simPath, snap, partType, fields, chunkNum=0, totNumChunks=1):\n",
    "    \"\"\" \n",
    "    Load part of a snapshot.\n",
    "    frm Dylan Nelson: https://www.tng-project.org/data/forum/topic/203/loading-the-tng100-1-data/\n",
    "    \"\"\"\n",
    "    nTypes = 6\n",
    "    ptNum = il.util.partTypeNum(partType)\n",
    "\n",
    "    with h5py.File(il.snapshot.snapPath(simPath,snap),'r') as f:\n",
    "        numPartTot = il.snapshot.getNumPart( dict(f['Header'].attrs.items()) )[ptNum]\n",
    "\n",
    "    # define index range\n",
    "    indRange_fullSnap = [0,numPartTot-1]\n",
    "    indRange = pSplitRange(indRange_fullSnap, totNumChunks, chunkNum, inclusive=True)\n",
    "\n",
    "    # load a contiguous chunk by making a subset specification in analogy to the group ordered loads\n",
    "    subset = { 'offsetType'  : np.zeros(nTypes, dtype='int64'),\n",
    "               'lenType'     : np.zeros(nTypes, dtype='int64') }\n",
    "\n",
    "    subset['offsetType'][ptNum] = indRange[0]\n",
    "    subset['lenType'][ptNum]    = indRange[1]-indRange[0]+1\n",
    "\n",
    "    # add snap offsets (as required)\n",
    "    with h5py.File(il.snapshot.offsetPath(simPath,snap),'r') as f:\n",
    "        subset['snapOffsets'] = np.transpose(f['FileOffsets/SnapByType'][()])\n",
    "\n",
    "    # load from disk\n",
    "    r = il.snapshot.loadSubset(simPath, snap, partType, fields, subset=subset)\n",
    "\n",
    "    return r\n",
    "\n",
    "def process_sim_chunk(snap_number,basePath,sim_to_use,nSubLoads,chunkIDs,T_h,T_c,c1s,c2e):\n",
    "    \"\"\"\n",
    "    processes part of the simulation on a single cpu. Is fed by unwrap_package().\n",
    "    this is specifically for pipes going along the x-axis from 0 to box length.\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    snap_number : [int] the snapshot number of the simulation to be processed\n",
    "    basePath    : [str] the path to the simulation data to be processed\n",
    "    sim_to_use  : [str] the simulation to be processed\n",
    "    nSubLoads   : [int] the total number of parts the simulation will be split into\n",
    "    chunkIDs    : [array of ints] the id numbers of the chunks of simulation to be processed on this cpu\n",
    "    T_h         : [= 10**7]  hot phase gase temperature [Kelvin] \n",
    "    T_c         : [= 10**3]  cold phase gas temperature [Kelvin]\n",
    "    c1s         : [0,pipe_width/2,pipe_width/2] coordinates at upper right of pipe start\n",
    "    c2e         : [0,-pipe_width/2,-pipe_width/2] coordinates at lower left of pipe end\n",
    "    \n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    verbose = False\n",
    "    \n",
    "    for i in range(len(chunkIDs)): #loop over all chunk ids\n",
    "        \n",
    "        chunkID = chunkIDs[i] # the chunk ID to be processed\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: chunkID = {0}'.format(chunkID))\n",
    "        \n",
    "        temp_dict = {} #initialise dictionary to store data for this chunk in\n",
    "        \n",
    "        #the name of the file dictionary for this chunk will be stored in \n",
    "        #CHARLIE NOTE (01/04/2022):\n",
    "        #currently hard-coded with _SpeedTest suffix which should be removed in final faster version\n",
    "        part_outdata_filename = '/u/cwalker/Illustris_Zhang_Method/temp_chunks/sim_{0:02d}_snap_{1:03d}_cID_{2}_SpeedTest.npy'.format(snap_number,chunkID,sim_to_use) \n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: store data at = {0}'.format(part_outdata_filename))\n",
    "            \n",
    "            \n",
    "        #initialise arrays to hold the desired information about\n",
    "        #the simulation cells in this part of the simulation\n",
    "\n",
    "        #pipe_cell_coords_part=[]\n",
    "        #pipe_cell_dens_part = []\n",
    "        #pipe_cell_elab_part=[]\n",
    "        #pipe_cell_sfr_part=[]\n",
    "        #pipe_cell_dark_part = []\n",
    "        #pipe_cell_warm_part=[]\n",
    "        #pipe_cell_pIDs_part=[]\n",
    "\n",
    "        ###########################\n",
    "        #load the partial data set#\n",
    "        ###########################\n",
    "\n",
    "        data = loadSubset(basePath,snap_number, 'gas', fields,chunkNum=chunkID, totNumChunks=nSubLoads)\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: data = {0}'.format(data))\n",
    "\n",
    "        #####################################\n",
    "        #create warm-phase gas mass fraction#\n",
    "        #####################################\n",
    "\n",
    "        density = data['Density'] #the density values along the light ray in gcm**-3\n",
    "        sfr     = data['StarFormationRate'] #the star formation rate along the light ray in g/s\n",
    "        ie      = data['InternalEnergy'] #the internal energy along the light ray in erg/g\n",
    "        ea      = data['ElectronAbundance'] #the electron abundance along the light ray\n",
    "        #calculate x and w, cold and warm phase gas mass fractions\n",
    "        x_frac = (temp2u(T_h,ea)-ie)/(temp2u(T_h,ea)-temp2u(T_c,ea)) #cold phase mass fraction\n",
    "        w_frac = 1 - x_frac # warm phase mass fraction\n",
    "        #only modify electron abundance if sfr = 0\n",
    "        w_frac[np.where(sfr==0)]=1\n",
    "        data['Warm']=w_frac    \n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: w frac = {0}'.format(data['Warm']))\n",
    "\n",
    "        ########################\n",
    "        #get cells in this pipe#\n",
    "        ########################\n",
    "\n",
    "        yz_pts = data['Coordinates'][:,[1,2]]\n",
    "        ur = c1s[1:] #upper right of pipe start (y and z only)\n",
    "        ll = c2e[1:] #lower left of pipe end (y and z only)\n",
    "        inidx = np.all((ll <= yz_pts) & (yz_pts <= ur), axis=1) #indexes of cells in pipe\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: inidx = {0}'.format(inidx))\n",
    "\n",
    "        ###########################\n",
    "        #get data of cells in pipe#\n",
    "        ###########################\n",
    "\n",
    "        #pipe_cell_coords_part.append(data['Coordinates'][inidx])       #coordinates [ckpc/h]\n",
    "        pipe_cell_coords_part=data['Coordinates'][inidx]      #coordinates [ckpc/h]\n",
    "\n",
    "        #pipe_cell_dens_part.append(data['Density'][inidx])           #densities [(1e10Msun/h)/(ckpc/h)**3]\n",
    "        pipe_cell_dens_part=data['Density'][inidx]          #densities [(1e10Msun/h)/(ckpc/h)**3]\n",
    "\n",
    "        #pipe_cell_elab_part.append(data['ElectronAbundance'][inidx]) #electron abundance [-]\n",
    "        pipe_cell_elab_part=data['ElectronAbundance'][inidx] #electron abundance [-]\n",
    "\n",
    "        #pipe_cell_sfr_part.append(data['StarFormationRate'][inidx]) #star formation rate [Msun/yr]\n",
    "        pipe_cell_sfr_part=data['StarFormationRate'][inidx] #star formation rate [Msun/yr]\n",
    "\n",
    "        #pipe_cell_dark_part.append(data['SubfindDMDensity'][inidx])  #comoving dark matter density [(1e10Msun/h)/(ckpc/h)**3]\n",
    "        pipe_cell_dark_part=data['SubfindDMDensity'][inidx]  #comoving dark matter density [(1e10Msun/h)/(ckpc/h)**3]\n",
    "\n",
    "        #pipe_cell_warm_part.append(data['Warm'][inidx])\n",
    "        pipe_cell_warm_part=data['Warm'][inidx]\n",
    "\n",
    "        #pipe_cell_pIDs_part.append(data['ParticleIDs'][inidx])\n",
    "        pipe_cell_pIDs_part=data['ParticleIDs'][inidx]\n",
    "        \n",
    "        #########################################\n",
    "        #store these to a dictionary to be saved#\n",
    "        #########################################\n",
    "        \n",
    "        temp_dict['Coordinates']       = pipe_cell_coords_part\n",
    "        temp_dict['Density']           = pipe_cell_dens_part\n",
    "        temp_dict['ElectronAbundance'] = pipe_cell_elab_part\n",
    "        temp_dict['StarFormationRate'] = pipe_cell_sfr_part\n",
    "        temp_dict['SubfindDMdensity']  = pipe_cell_dark_part\n",
    "        temp_dict['Warm']              = pipe_cell_warm_part\n",
    "        temp_dict['ParticleIDs']       = pipe_cell_pIDs_part\n",
    "        \n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: temp dict = {0}'.format(temp_dict))\n",
    "\n",
    "        #####################################################################\n",
    "        #save data to temporary array for loading with the rest of the parts#\n",
    "        #####################################################################\n",
    "        np.save('{0}'.format(part_outdata_filename),temp_dict)\n",
    "\n",
    "        if verbose == True:\n",
    "            print('Verbose mode check: saved')\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "def unwrap_package(package):\n",
    "    \"\"\"\n",
    "    Helper function for parsing simulation in parallel using multiprocessing.\n",
    "    Unpacks the set of data necessary for parsing the simulation.\n",
    "    Then parses the simulation using process_sim_chunk().\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    package : a list containing the input data, which are X arguments in the following order:\n",
    "    \n",
    "        snap_number : [int] the snapshot number of the simulation to be processed\n",
    "        basePath    : [str] the path to the simulation data to be processed\n",
    "        sim_to_use  : [str] the simulation to be processed\n",
    "        nSubLoads   : [int] the total number of parts the simulation will be split into\n",
    "        chunkIDs    : [array of ints] the id numbers of the chunks of simulation to be processed on this cpu\n",
    "        T_h         : [= 10**7]  hot phase gase temperature [Kelvin] \n",
    "        T_c         : [= 10**3]  cold phase gas temperature [Kelvin]\n",
    "        c1s         : [0,pipe_width/2,pipe_width/2] coordinates at upper right of pipe start\n",
    "        c2e         : [0,-pipe_width/2,-pipe_width/2] coordinates at lower left of pipe end\n",
    "    \n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    output of process_package()\n",
    "    \"\"\"\n",
    "    \n",
    "    verbose=False\n",
    "    \n",
    "    #unwrap the package to feed to process_sim_chunk()\n",
    "    snap_number = package[0]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: snap_number = {0}'.format(snap_number))\n",
    "    basePath    = package[1]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: basePath = {0}'.format(basePath))\n",
    "    sim_to_use  = package[2]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: sim_to_use = {0}'.format(sim_to_use))\n",
    "    nSubLoads   = package[3]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: nSubLoads = {0}'.format(nSubLoads))\n",
    "    chunkIDs    = package[4]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: chunkIDs = {0}'.format(chunkIDs))\n",
    "    T_h         = package[5]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: T_h = {0}'.format(T_h))\n",
    "    T_c         = package[6]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: T_c = {0}'.format(T_c))\n",
    "    c1s         = package[7]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: c1s = {0}'.format(c1s))\n",
    "    c2e         = package[8]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: c2e = {0}'.format(c2e))\n",
    "    \n",
    "    print('torun: ',snap_number,basePath,sim_to_use,nSubLoads,chunkIDs,T_h,T_c,c1s,c2e)\n",
    "    \n",
    "    \n",
    "    #run process_sim_chunk()\n",
    "    process_sim_chunk(snap_number,basePath,sim_to_use,nSubLoads,chunkIDs,T_h,T_c,c1s,c2e)\n",
    "    \n",
    "    return 'done'\n",
    "\n",
    "def oldpIDshIDconverter(pipe_cell_pIDs,AllPartIDs,AllSubhIDs):\n",
    "    \"\"\"\n",
    "    Old version of the code which took a set of particle IDs and created a list of \n",
    "    corresponding subhalo IDs\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    pipe_cell_pIDs : the particle ids of cells in a given pipe\n",
    "    AllPartIDs     : particle ID list for all cells in the desired simulation\n",
    "    AllSubhIDs     : subhalo ID list for all cells in the desired simulation\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    pipe_cell_shIDs : the corresponding subhalo IDs for every particle id in the cell.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #print('Conversion check')\n",
    "\n",
    "    #create a set of particle IDs for the cells in this pipe\n",
    "    PartID_Set = set(pipe_cell_pIDs.tolist())\n",
    "\n",
    "    #initialise an array to contain the corresponding positions within the simulation of these cells\n",
    "    sim_inds = np.zeros(pipe_cell_pIDs.shape,dtype=int)\n",
    "\n",
    "    #loop over all particle IDs in the desired simulation\n",
    "    for i, x in enumerate(AllPartIDs):\n",
    "\n",
    "        #find when particle ID is also in the pipe\n",
    "        if x in PartID_Set:\n",
    "\n",
    "            #find where that particle ID is in the pipe\n",
    "            pipe_idx = np.where(pipe_cell_pIDs==x)\n",
    "\n",
    "            #assign the pipe at that point the cell's corresponding simulation position\n",
    "            sim_inds[pipe_idx] = i\n",
    "\n",
    "\n",
    "    #for all of these simulation positions, get the correct subhalo ID\n",
    "    pipe_cell_shIDs = np.array(AllSubhIDs[sim_inds])\n",
    "    #print(pipe_cell_shIDs)\n",
    "\n",
    "    #print('Conversion check end')\n",
    "\n",
    "    return pipe_cell_shIDs\n",
    "\n",
    "def newpIDshIDconverter(pipe_cell_pIDs,ChunkedPartIDs,ChunkedSubhIDs):\n",
    "    \"\"\"\n",
    "    New version of the code which creates a set of subhalo IDs from a set of particle\n",
    "    IDs.\n",
    "    \n",
    "    This version loops through each chunk of the simulation ID lists in turn searching\n",
    "    for relevant particle and subhalo IDs. \n",
    "    \n",
    "    Note: could be improved to be faster if, when all correct particle IDs are found, it\n",
    "    does not need to search further chunks. This is not yet implemented.\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    pipe_cell_pIDs : the particle ids of cells in a given pipe\n",
    "    ChunkedPartIDs : list containing locations of the chunks of \n",
    "                     the particle ID list for all cells in the \n",
    "                     desired simulation. If all of these were loaded\n",
    "                     into a single array, the result would be the\n",
    "                     same as AllPartIDs in oldpIDshIDconverter().\n",
    "    ChunkedSubhIDs : list containing locations of the chunks of \n",
    "                     the subhalo ID list for all cells in the \n",
    "                     desired simulation. If all of these were loaded\n",
    "                     into a single array, the result would be the\n",
    "                     same as AllPartIDs in oldpIDshIDconverter().\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    pipe_cell_shIDs : the corresponding subhalo IDs for every particle IDs.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #create a set of particle IDs for the cells in this pipe\n",
    "    PartID_Set = set(pipe_cell_pIDs.tolist())\n",
    "    \n",
    "    #initialise an array to contain all subhalo IDs in simulation\n",
    "    pipe_cell_shIDs = np.ones(pipe_cell_pIDs.shape,dtype=int)*-1\n",
    "    \n",
    "    #print(ChunkedPartIDs)\n",
    "    \n",
    "    #load chunks of the all-simulation particle ID list\n",
    "    for i in range(len(ChunkedPartIDs)):\n",
    "        \n",
    "        #True/False array of same shape as pipe data which allows \n",
    "        #us to extract the relevant IDs from each chunk\n",
    "        TF_arr = np.full(pipe_cell_pIDs.shape, False) #begin with false, flip to true when chunk contains ID\n",
    "        \n",
    "        sim_inds = []#initialise an array to contain  positions of any cells in this chunk\n",
    "        \n",
    "        #get location of chunks to load\n",
    "        PartFile_toload = ChunkedPartIDs[i] #all-simulation particle ID list chunk\n",
    "        SubhFile_toload = ChunkedSubhIDs[i] #all-simulation subhalo ID list chunk\n",
    "        \n",
    "        #load the ID list chunks\n",
    "        ChunkOfPartIDs = np.load(PartFile_toload) #particle chunk\n",
    "        ChunkOfSubhIDs = np.load(SubhFile_toload) #subhalo chunk\n",
    "\n",
    "        #loop over the particle IDs in the chunk\n",
    "        for j, x in enumerate(ChunkOfPartIDs):\n",
    "            \n",
    "            #find if particle ID is also in the pipe\n",
    "            if x in PartID_Set:\n",
    "                                \n",
    "                #find where that particle ID is in the pipe\n",
    "                pipe_idx = np.where(pipe_cell_pIDs==x)\n",
    "                \n",
    "                #flip the True/False array index to True for this cell\n",
    "                TF_arr[pipe_idx] = True\n",
    "                \n",
    "                #append the cell's corresponding chunk position\n",
    "                sim_inds.append(j)\n",
    "                \n",
    "                #print(i,j,pipe_idx,x,ChunkOfSubhIDs[j])\n",
    "\n",
    "            \n",
    "        #convert all chunk position indices to array\n",
    "        sim_inds = np.array(sim_inds)\n",
    "        \n",
    "        #record all corresponding subhalo IDs in this chunk\n",
    "        #print(pipe_cell_shIDs[TF_arr])\n",
    "        #print(sim_inds)\n",
    "        #print(ChunkOfSubhIDs[sim_inds])\n",
    "        if sim_inds.size>0:#only try this if the array is not empty\n",
    "            pipe_cell_shIDs[TF_arr] = ChunkOfSubhIDs[sim_inds]\n",
    "            \n",
    "\n",
    "    return pipe_cell_shIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa54e2f",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3de674",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#initialise#\n",
    "############\n",
    "\n",
    "sim_to_use = sim_to_use\n",
    "print('Simulation to use will be: {0}'.format(sim_to_use))\n",
    "\n",
    "pipes_per_snap = pipes_per_snap\n",
    "print('Number of pipes to create per snapshot: {0}'.format(pipes_per_snap))\n",
    "\n",
    "snaps_to_process = [snap_to_process]#,13,11,8,6,4,3,2]\n",
    "print('Snapshots to process will be {0}'.format(snaps_to_process))\n",
    "\n",
    "#The number of cells in the chosen snapshot\n",
    "#ncells = dataPT0['Coordinates'].shape[0]\n",
    "#print('Number of cells in snapshot {0} is {1}'.format(snap_number,ncells))\n",
    "\n",
    "#The width of the pipe\n",
    "pipe_width = 200 #By following zhang+20 definition, sides will be 200ckpc/h in length\n",
    "print('Pipe width will be {0} ckpc/h'.format(pipe_width))\n",
    "\n",
    "#The number of bins along a single line of sight\n",
    "nbins=10000 #Zhang+20 definition: 10,000\n",
    "print('There will be {0} bins on each sightline'.format(nbins))\n",
    "\n",
    "#Define the mass of a proton for dDM/dz calculations\n",
    "protonmass = c.m_p.to('kg')\n",
    "print('Proton mass is {0}'.format(protonmass))\n",
    "\n",
    "#Define the hydrogen mass fraction for dDM/dz calculations\n",
    "hmassfrac = 3./4.\n",
    "print('Chosen H mass fraction is {0}. Check whether this is correct'.format(hmassfrac))\n",
    "\n",
    "#calculate the critical density at redshift zero for structure categorisation\n",
    "#source to formula: https://astronomy.swin.edu.au/cosmos/c/Critical+Density\n",
    "grav=c.G.to('m**3/(kg*s**2)') #g as a YT quantity in correct units\n",
    "H=cosmosource.H(0).to('km/s/Mpc') #hubble const at z=0 in km/s/Mpc\n",
    "my_dens_crit = ((3 * H**2)/(8*np.pi* grav)).to('kg/m**3')\n",
    "print('Critical density at z=0 = {0}'.format(my_dens_crit))\n",
    "\n",
    "nSubLoads = 100 #number of subloads to split simulation into\n",
    "\n",
    "##pipe info for test\n",
    "#npipes      = 1  #number of pipes to create\n",
    "#snap_number = 99 #snapshot number for test\n",
    "\n",
    "\n",
    "#base path to simulation\n",
    "#basePath = '/virgo/simulations/IllustrisTNG/{0}/output/'.format(sim_to_use)\n",
    "basePath = '/ptmp/cwalker/Illustris_FRB_Project/TNG_copies/virgo/simulations/IllustrisTNG/{0}/output/'.format(sim_to_use)\n",
    "#basePath = '/virgo/simulations/IllustrisTNG/{0}/output/'.format(sim_to_use)\n",
    "\n",
    "#load header\n",
    "#header = il.groupcat.loadHeader(basePath,snap_number)\n",
    "\n",
    "#fields to load for test\n",
    "fields=['Density',\n",
    "        'ElectronAbundance',\n",
    "        'StarFormationRate',\n",
    "        'InternalEnergy',\n",
    "        'Coordinates',\n",
    "        'Masses',\n",
    "        'SubfindDMDensity',\n",
    "        'ParticleIDs'] \n",
    "\n",
    "#define constants foor warm-phase gas mass fraction calculation\n",
    "T_h = 10**7  #hot phase gase temperature [Kelvin]\n",
    "T_c = 10**3  #cold phase gas temperature [Kelvin]\n",
    "x_h = 0.75   #Hydrogen mass fraction\n",
    "\n",
    "#identify number of available cores on the system\n",
    "ncpus = m.cpu_count()\n",
    "\n",
    "#choose the number of cores to use at once. \n",
    "cpus_to_use = cpus_to_use \n",
    "\n",
    "#calculate the number of full core runs to be used to check for simulation cells in pipe\n",
    "#this number is the number of parts of the simulation which will be loaded simultaneously\n",
    "n_full_core = nSubLoads//cpus_to_use\n",
    "\n",
    "#calculate the number of cores which must be used to check the remaining simulation cells\n",
    "#this number is the number of leftover parts of the simulation which will be loaded all at once\n",
    "n_partial_core = nSubLoads%cpus_to_use\n",
    "\n",
    "print('To parse simulation data, {0} cpus will load data simultaneously. This will happen {1} times. The remaining data needs {2} cpus. These will be loaded simultaneously.'.format(cpus_to_use,n_full_core,n_partial_core))\n",
    "\n",
    "#if statement to allow testing of whether multiproccessing-related functions are working correctly\n",
    "#If it is set to False, multiprocessing is enabled.\n",
    "#If set to true, everything is done sequentially with no multiprocessing.\n",
    "parallelcodetest = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e27589",
   "metadata": {},
   "source": [
    "# Pipe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "########################\n",
    "##ACTUAL PIPE CREATION##\n",
    "########################\n",
    "########################\n",
    "\n",
    "pIDshID_version = 'speed' #a switch while testing the new vs old shID versions, can be old, new, both, speed\n",
    "\n",
    "\n",
    "#####################\n",
    "#Loop over snapshots#\n",
    "#####################\n",
    "\n",
    "for snapshot_to_process in range(len(snaps_to_process)):\n",
    "   \n",
    "    ############\n",
    "    #initialise#\n",
    "    ############\n",
    "    \n",
    "    npipes            = pipes_per_snap  #number of pipes to create\n",
    "    snap_number       = snaps_to_process[snapshot_to_process] #snapshot number for test\n",
    "    \n",
    "    print('Currently processing snapshot: {0}'.format(snap_number))\n",
    "    \n",
    "    \n",
    "    #############\n",
    "    #load header#\n",
    "    #############\n",
    "    \n",
    "    header = il.groupcat.loadHeader(basePath,snap_number)\n",
    "    print('Header for snap = {0}'.format(header))\n",
    "    \n",
    "    #####################################################################################################\n",
    "    #edit for larger simulations: put this in an if statement to test both chunkeed and whole matchlists#\n",
    "    #####################################################################################################\n",
    "    \n",
    "    #load the particle and subhalo ID lists for all cells in the desired simulation\n",
    "    if pIDshID_version == 'old':\n",
    "        #load the whole matchlists\n",
    "        AllPartIDs = np.load('/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/Matchlist_dir_{0}/PartList_Snap{1}.npy'.format(sim_to_use,snap_number))\n",
    "        AllSubhIDs = np.load('/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/Matchlist_dir_{0}/ShIDList_Snap{1}.npy'.format(sim_to_use,snap_number))\n",
    "    \n",
    "    elif pIDshID_version == 'new':\n",
    "        #get list of matchlist chunks\n",
    "        print('Using new version of code which loads particle/subhalo matchlists in chunks from /ptmp/')\n",
    "        Chunked_loc = '/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/'.format(sim_to_use) #location of the chunked data\n",
    "        \n",
    "        #get the particle ID list chunks\n",
    "        ChunkedPartIDs = os.listdir(Chunked_loc)\n",
    "        ChunkedPartIDs = ['{0}/{1}'.format(Chunked_loc,i) for i in ChunkedPartIDs if 'PartList_Snap{0}_Chunk'.format(snap_number) in i]\n",
    "        ChunkedPartIDs.sort()\n",
    "        \n",
    "        #get the subhalo ID list chunks\n",
    "        ChunkedSubhIDs = os.listdir(Chunked_loc)\n",
    "        ChunkedSubhIDs = ['{0}/{1}'.format(Chunked_loc,i) for i in ChunkedSubhIDs if 'ShIDList_Snap{0}_Chunk'.format(snap_number) in i]\n",
    "        ChunkedSubhIDs.sort()\n",
    "        \n",
    "    elif pIDshID_version == 'both':\n",
    "        print('Checking output of new and old subhalo ID generation methods...')\n",
    "        #load the whole matchlists\n",
    "        AllPartIDs = np.load('/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/Matchlist_dir_{0}/PartList_Snap{1}.npy'.format(sim_to_use,snap_number))\n",
    "        AllSubhIDs = np.load('/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/Matchlist_dir_{0}/ShIDList_Snap{1}.npy'.format(sim_to_use,snap_number))        \n",
    "\n",
    "        #get list of matchlist chunks\n",
    "        Chunked_loc = '/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/'.format(sim_to_use) #location of the chunked data\n",
    "        print('Location of Particle ID matchlist chunks: {0}'.format(Chunked_loc))\n",
    "        \n",
    "        #get the particle ID list chunks\n",
    "        ChunkedPartIDs = os.listdir(Chunked_loc)\n",
    "        #print('All files in the location: {0}'.format(ChunkedPartIDs))\n",
    "        print('Testing for string: {0}'.format('PartList_Snap{0}_Chunk'.format(snap_number)))\n",
    "        #print([i for i in ChunkedPartIDs if 'PartList_Snap{0}_Chunk'.format(snap_number) in i])\n",
    "        ChunkedPartIDs = ['{0}/{1}'.format(Chunked_loc,i) for i in ChunkedPartIDs if 'PartList_Snap{0}_Chunk'.format(snap_number) in i]\n",
    "        ChunkedPartIDs.sort()\n",
    "        #print('Particle ID matchlist chunks: {0}'.format(ChunkedPartIDs))\n",
    "        \n",
    "        #get the subhalo ID list chunks\n",
    "        ChunkedSubhIDs = os.listdir(Chunked_loc)\n",
    "        ChunkedSubhIDs = ['{0}/{1}'.format(Chunked_loc,i) for i in ChunkedSubhIDs if 'ShIDList_Snap{0}_Chunk'.format(snap_number) in i]\n",
    "        ChunkedSubhIDs.sort()\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    ##Check that file to store data dictionary in exists.##\n",
    "    ##If it doesn't, create it and initialise it.        ##\n",
    "    ##If it does, load it.                               ##\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    \n",
    "    #outfile_name = '/u/cwalker/Illustris_Zhang_Method/Sim_{0}_Snap_{1}_dDMdz_Output_pID_test.npy'.format(sim_to_use,snap_number) #name of this file\n",
    "    \n",
    "    #CHARLIE NOTE (01/04/2022):\n",
    "    #currently hard-coded with _SpeedTest subdir and suffix which should be removed in final faster version\n",
    "    outfile_name = '/u/cwalker/Illustris_Zhang_Method/SpeedTest/Sim_{0}_Snap_{1}_dDMdz_Output_SpeedTest.npy'.format(sim_to_use,snap_number) #name of this file (in separate speed directory)\n",
    "\n",
    "     \n",
    "    #####################################\n",
    "    #check to see if file already exists#\n",
    "    #####################################\n",
    "        \n",
    "    if not os.path.isfile('{0}'.format(outfile_name)):\n",
    "        print('Warning: file {0} does not yet exist. Must be created.'.format(outfile_name))\n",
    "        existcheck = False\n",
    "    else:\n",
    "        print('Note: File {0} already exists. Will be loaded.'.format(outfile_name))\n",
    "        existcheck = True\n",
    "        \n",
    "    ######################\n",
    "    #if file exists, load#\n",
    "    ######################\n",
    "    if existcheck == True:\n",
    "        print('Loading file ({0})'.format(outfile_name))\n",
    "        dict_to_edit = np.load(outfile_name,allow_pickle=True).tolist()\n",
    "        print('File loaded has {0} keys'.format(len(dict_to_edit)))\n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    #if file doesn't exist, create and initialise#\n",
    "    ##############################################\n",
    "    \n",
    "    if existcheck == False:\n",
    "        print('Creating file ({0})'.format(outfile_name))\n",
    "        dict_to_edit = {} # initialise\n",
    "        \n",
    "        #initialise keys to be stored\n",
    "        dict_to_edit['dDMdz_Zhang'] = []\n",
    "        dict_to_edit['dDMdzHalo_Zhang'] = []\n",
    "        dict_to_edit['dDMdzFilament_Zhang'] = []\n",
    "        dict_to_edit['dDMdzVoid_Zhang'] = []\n",
    "        dict_to_edit['nHalo_Zhang'] = []\n",
    "        dict_to_edit['nFilament_Zhang'] = []\n",
    "        dict_to_edit['nVoid_Zhang'] = []\n",
    "\n",
    "        dict_to_edit['dDMdz_Pakmor'] = []\n",
    "        dict_to_edit['dDMdzHalo_Pakmor'] = []\n",
    "        dict_to_edit['dDMdzFilament_Pakmor'] = []\n",
    "        dict_to_edit['dDMdzVoid_Pakmor'] = []\n",
    "        dict_to_edit['nHalo_Pakmor'] = []\n",
    "        dict_to_edit['nFilament_Pakmor'] = []\n",
    "        dict_to_edit['nVoid_Pakmor'] = []\n",
    "        \n",
    "        #edit 09/02/22 for storing information about subhalos along sightline\n",
    "        dict_to_edit['firstShID'] = [] #first subhalo ID number along pipe line of sight\n",
    "        dict_to_edit['uniqueShIDs'] = [] #unique subhalo ID numbers along pipe line of sight\n",
    "        dict_to_edit['closestCoords'] = [] #closest coordinates along pipe line of sight to these subhalos\n",
    "        \n",
    "        #save\n",
    "        np.save('{0}'.format(outfile_name),dict_to_edit)\n",
    "        print('File created and initialised')\n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    #check to see if file contains correct number of pipes#\n",
    "    #######################################################\n",
    "    \n",
    "    if len(dict_to_edit['dDMdz_Pakmor'])<npipes:\n",
    "        print('Warning: File currently contains too few pipes ({0}/{1})'.format(len(dict_to_edit['dDMdz_Pakmor']),npipes))\n",
    "        lencheck = False\n",
    "    \n",
    "    elif len(dict_to_edit['dDMdz_Pakmor'])==npipes:\n",
    "        print('Warning: File already contains the correct number of pipes ({0}). No more will be created'.format(len(dict_to_edit['dDMdz_Pakmor'])))\n",
    "        lencheck = True\n",
    "    \n",
    "    ###################################################################\n",
    "    #if number of pipes is too low, calculate how many more are needed#\n",
    "    ###################################################################\n",
    "    \n",
    "    if lencheck == False:\n",
    "        new_npipes = npipes - len(dict_to_edit['dDMdz_Pakmor'])\n",
    "        print('Remaining number of pipes needed is: {0}'.format(new_npipes))\n",
    "    \n",
    "    #####################################\n",
    "    #if number of pipes is correct, exit#\n",
    "    #####################################\n",
    "    \n",
    "    if lencheck == True:\n",
    "        print('No new pipes needed. Quitting program.')\n",
    "        #break\n",
    "\n",
    "    \n",
    "    ###########################\n",
    "    ###########################\n",
    "    ##create pipes, get dDMdz##\n",
    "    ###########################\n",
    "    ###########################\n",
    "    elif lencheck==False:\n",
    "        while(len(dict_to_edit['dDMdz_Pakmor'])<npipes): #while not enough pipes have been created:\n",
    "\n",
    "            #############\n",
    "            #Create Pipe#\n",
    "            #############\n",
    "\n",
    "            #HACK FOR TESTING TO MAKEE SURE IT ONLY LOOPS ONCE. REMOVE BEFORRE PUTTING IN SCRIPT!\n",
    "            #npipes = 0\n",
    "\n",
    "            #########################################\n",
    "            #define los coordinates at start of pipe#\n",
    "            #########################################\n",
    "\n",
    "            #By Zhang+20 definition of following x-axis,\n",
    "            #x will be zero, y and z will be random\n",
    "            #units default = ckpc/h (compare box size to https://www.tng-project.org/about/)\n",
    "\n",
    "            pipe_start_coords = np.array([0,\n",
    "                                 np.random.uniform(0,header['BoxSize'],1)[0],\n",
    "                                 np.random.uniform(0,header['BoxSize'],1)[0]])\n",
    "            #print('Random start cell coordinates: {0}'.format(pipe_start_coords))\n",
    "\n",
    "            ###################################\n",
    "            #define coordinates at end of pipe#\n",
    "            ###################################\n",
    "\n",
    "            #By Zhang+20 definition of following x-axis,\n",
    "            #x will be length of simulation,y and z will be same as start coords\n",
    "\n",
    "            pipe_end_coords = pipe_start_coords+np.array([header['BoxSize'],0,0])\n",
    "            #print('Pipe end cell coordinates: {0}'.format(pipe_end_coords))\n",
    "\n",
    "\n",
    "            ########################\n",
    "            #plot the line of sight#\n",
    "            ########################\n",
    "\n",
    "            los_toplot=list(zip(pipe_start_coords,pipe_end_coords))\n",
    "\n",
    "            ########################\n",
    "            #construct pipe corners#\n",
    "            ########################\n",
    "\n",
    "            #Add and subtract half of pipe length from y and z coords for y and z boundaries\n",
    "            #code adapted from https://stackoverflow.com/questions/33540109/plot-surfaces-on-a-cube\n",
    "\n",
    "            c1s = pipe_start_coords + np.array([0,pipe_width/2,pipe_width/2]) #start corner 1\n",
    "            c2s = pipe_start_coords + np.array([0,-pipe_width/2,-pipe_width/2]) #start corner 2\n",
    "            c3s = pipe_start_coords + np.array([0,pipe_width/2,-pipe_width/2]) #start corner 3\n",
    "            c4s = pipe_start_coords + np.array([0,-pipe_width/2,pipe_width/2]) #start corner 4\n",
    "\n",
    "            c1e = pipe_end_coords + np.array([0,pipe_width/2,pipe_width/2]) #end corner 1\n",
    "            c2e = pipe_end_coords + np.array([0,-pipe_width/2,-pipe_width/2]) #end corner 2\n",
    "            c3e = pipe_end_coords + np.array([0,pipe_width/2,-pipe_width/2]) #end corner 3\n",
    "            c4e = pipe_end_coords + np.array([0,-pipe_width/2,pipe_width/2]) #end corner 4\n",
    "\n",
    "            corners = np.array([c1s,c2s,c3s,c4s,c1e,c2e,c3e,c4e])\n",
    "\n",
    "            ######################\n",
    "            #construct pipe edges#\n",
    "            ######################\n",
    "\n",
    "            line1 = list(zip(c1s,c1e))\n",
    "            line2 = list(zip(c2s,c2e))\n",
    "            line3 = list(zip(c3s,c3e))\n",
    "            line4 = list(zip(c4s,c4e))\n",
    "            line5 = list(zip(c1s,c3s))\n",
    "            line6 = list(zip(c3s,c2s))\n",
    "            line7 = list(zip(c2s,c4s))\n",
    "            line8 = list(zip(c4s,c1s))\n",
    "            line9 = list(zip(c1e,c3e))\n",
    "            line10 = list(zip(c3e,c2e))\n",
    "            line11 = list(zip(c2e,c4e))\n",
    "            line12 = list(zip(c4e,c1e))\n",
    "\n",
    "            lines_todraw = np.array([line1,line2,line3,line4,line5,line6,line7,line8,line9,line10,line11,line12])\n",
    "\n",
    "            ###########################################\n",
    "            #get cells in this pipe by partial loading#\n",
    "            ###########################################\n",
    "\n",
    "            ###########################################\n",
    "            ###########################################\n",
    "            ##Parallelisation edit of the code begins##\n",
    "            ###########################################\n",
    "            ###########################################\n",
    "\n",
    "            \n",
    "            if parallelcodetest == True:\n",
    "                print('Running non parallel version of code to test functions')\n",
    "                \n",
    "            #test  of functions\n",
    "                cpu_map_a = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core)\n",
    "                cpu_map_b = np.arange(n_full_core*cpus_to_use,nSubLoads).reshape(n_partial_core,1)\n",
    "                package_a = [(snap_number,basePath,sim_to_use,nSubLoads,cpu_map_a[i],T_h,T_c,c1s,c2e) for i in range(cpus_to_use)]\n",
    "                package_b = [(snap_number,basePath,sim_to_use,nSubLoads,cpu_map_b[i],T_h,T_c,c1s,c2e) for i in range(n_partial_core)]\n",
    "                print('Testing CPU maps: A: {0}\\n B: {1}'.format(cpu_map_a,cpu_map_b))\n",
    "                print('Testing packages: A: {0}\\n B: {1}'.format(package_a,package_b))\n",
    "                #running one package through the code\n",
    "                print('Package to run (A): {0}'.format(package_a[0]))\n",
    "                print('Package to run (B): {0}'.format(package_b[0]))\n",
    "                print('\\nrunning A...\\n')\n",
    "                for test_i in range(len(package_a)):\n",
    "                    unwrap_package(package_a[test_i])\n",
    "                print('\\nrunning B...\\n')\n",
    "                for test_i in range(len(package_b)):\n",
    "                    unwrap_package(package_b[test_i])\n",
    "                print('\\nRan successfully')\n",
    "                \n",
    "            elif parallelcodetest==False:\n",
    "                print('Running parallelised version to check parallelisation')\n",
    "            \n",
    "                #create cpu_map and packages for processing\n",
    "                #this array dictates which sections of the data a cpu will load\n",
    "\n",
    "                if n_partial_core ==0: #if there are no remaining parts to load after the full core runs:\n",
    "                    #cpu map\n",
    "                    cpu_map = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core)\n",
    "                    #the package to be unwrapped for multiprocessing\n",
    "                    package = [(snap_number,basePath,sim_to_use,nSubLoads,cpu_map[i],T_h,T_c,c1s,c2e) for i in range(cpus_to_use)]\n",
    "\n",
    "                    print(cpu_map)\n",
    "\n",
    "                    with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "                        run = p.map(unwrap_package,package,chunksize=1) #run the multiprocessing\n",
    "                    p.terminate() #terminate after completion\n",
    "\n",
    "                elif n_partial_core > 0: #if there are remaining parts to load after the full core runs:\n",
    "                    #cpu map for full core runs\n",
    "                    cpu_map_a = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core)\n",
    "                    #package for full core runs\n",
    "                    package_a = [(snap_number,basePath,sim_to_use,nSubLoads,cpu_map_a[i],T_h,T_c,c1s,c2e) for i in range(cpus_to_use)]\n",
    "                    #cpu map for partial core run\n",
    "                    cpu_map_b = np.arange(n_full_core*cpus_to_use,nSubLoads).reshape(n_partial_core,1)\n",
    "                    #package for full core runs\n",
    "                    package_b = [(snap_number,basePath,sim_to_use,nSubLoads,cpu_map_b[i],T_h,T_c,c1s,c2e) for i in range(n_partial_core)]\n",
    "\n",
    "                    print('a',cpu_map_a,package_a,'b',cpu_map_b,package_b)\n",
    "\n",
    "                    #full core multiprocessing\n",
    "                    print('full core')\n",
    "                    with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "                        run = p.map(unwrap_package,package_a,chunksize=1) #run the multiprocessing\n",
    "                    p.terminate() #terminate after completion\n",
    "\n",
    "                    #partial core multiprocessing\n",
    "                    print('partial core')\n",
    "                    with closing(Pool(n_partial_core)) as p: #invoke multiproccessing\n",
    "                        run = p.map(unwrap_package,package_b,chunksize=1) #run the multiprocessing\n",
    "                    p.terminate() #terminate after completion\n",
    "            \n",
    "            \n",
    "            ###########################################################\n",
    "            #loop over stored temporary files for each part, load data#\n",
    "            ###########################################################\n",
    "            \n",
    "            #initialise arrays to hold all loaded data\n",
    "\n",
    "            all_coords = [] #coordinates\n",
    "            all_dens  = [] #density\n",
    "            all_elab  = [] #electron abundance\n",
    "            all_sfr   = [] #star formation rate\n",
    "            all_dark  = [] #dark matter density\n",
    "            all_warm  = [] #warm phase gas mass fraction\n",
    "            all_pIDs  = [] #particle ID number\n",
    "            \n",
    "            for i in range(nSubLoads): #loop over parts\n",
    "\n",
    "                #load file\n",
    "                #CHARLIE NOTE (01/04/2022):\n",
    "                #currently hard-coded with _SpeedTest suffix which should be removed in final faster version\n",
    "                toload_filename = '/u/cwalker/Illustris_Zhang_Method/temp_chunks/sim_{0:02d}_snap_{1:03d}_cID_{2}_SpeedTest.npy'.format(snap_number,i,sim_to_use) \n",
    "                loaded_dict = np.load(toload_filename,allow_pickle=True).tolist()\n",
    "\n",
    "                #append data to array\n",
    "                all_coords.append(loaded_dict['Coordinates'])\n",
    "                all_dens.append(loaded_dict['Density'])\n",
    "                all_elab.append(loaded_dict['ElectronAbundance'])\n",
    "                all_sfr.append(loaded_dict['StarFormationRate'])\n",
    "                all_dark.append(loaded_dict['SubfindDMdensity'])\n",
    "                all_warm.append(loaded_dict['Warm'])\n",
    "                all_pIDs.append(loaded_dict['ParticleIDs'])\n",
    "                \n",
    "                #remove temporary file after loading\n",
    "                os.remove(toload_filename)\n",
    "\n",
    "            #############################\n",
    "            #flatten into correct format#\n",
    "            #############################\n",
    "\n",
    "            pipe_cell_coords = np.array([item for sublist in all_coords for item in sublist])\n",
    "            pipe_cell_dens = np.array([item for sublist in all_dens for item in sublist])\n",
    "            pipe_cell_elab = np.array([item for sublist in all_elab for item in sublist])\n",
    "            pipe_cell_sfr = np.array([item for sublist in all_sfr for item in sublist])\n",
    "            pipe_cell_dark = np.array([item for sublist in all_dark for item in sublist])\n",
    "            pipe_cell_warm = np.array([item for sublist in all_warm for item in sublist])\n",
    "            pipe_cell_pIDs = np.array([item for sublist in all_pIDs for item in sublist])\n",
    "            \n",
    "            \n",
    "            ##################################\n",
    "            ##################################\n",
    "            ##parallelisation edit ends here##\n",
    "            ##################################\n",
    "            ##################################\n",
    "\n",
    "            ############################\n",
    "            ############################\n",
    "            ##partial load insert ends##\n",
    "            ############################\n",
    "            ############################\n",
    "            \n",
    "            ############################\n",
    "            ############################\n",
    "            ##subhalo ID insert begins##\n",
    "            ############################\n",
    "            ############################\n",
    "            \n",
    "            #####################################\n",
    "            #Convert particle IDs to subhalo ids#\n",
    "            #####################################\n",
    "            \n",
    "            if pIDshID_version == 'old':\n",
    "                print('old shID code')\n",
    "                #run old version of the particle ID to subhalo ID conversion\n",
    "                pipe_cell_shIDs = oldpIDshIDconverter(pipe_cell_pIDs,AllPartIDs,AllSubhIDs)\n",
    "                \n",
    "            elif pIDshID_version == 'new':\n",
    "                print('new shID code')\n",
    "                #run the new version of the particle ID to subhalo ID conversion\n",
    "                pipe_cell_shIDs = newpIDshIDconverter(pipe_cell_pIDs,ChunkedPartIDs,ChunkedSubhIDs)\n",
    "                \n",
    "            elif pIDshID_version == 'both':\n",
    "                print('comparing both shID codes')\n",
    "                #run old version of the particle ID to subhalo ID conversion\n",
    "                pipe_cell_shIDs_1 = oldpIDshIDconverter(pipe_cell_pIDs,AllPartIDs,AllSubhIDs)\n",
    "                #run the new version of the particle ID to subhalo ID conversion\n",
    "                pipe_cell_shIDs_2 = newpIDshIDconverter(pipe_cell_pIDs,ChunkedPartIDs,ChunkedSubhIDs)\n",
    "                #compare both versions\n",
    "                array_equal_test = np.array_equal(pipe_cell_shIDs_1,pipe_cell_shIDs_2)\n",
    "                if array_equal_test==True:\n",
    "                    print('Arrays are the same')\n",
    "                    pipe_cell_shIDs = pipe_cell_shIDs_1 #post-check, rename to carry on with code.\n",
    "                else:\n",
    "                    print('error: results are not the same')\n",
    "                    print(pipe_cell_shIDs_1,pipe_cell_shIDs_2)\n",
    "                    break\n",
    "            elif pIDshID_version=='speed':\n",
    "                print('speed version chosen. shIDs will be given placeholder values of 1')\n",
    "                pipe_cell_shIDs = np.ones_like(pipe_cell_pIDs)\n",
    "                    \n",
    "            ##########################\n",
    "            ##########################\n",
    "            ##subhalo ID insert ends##\n",
    "            ##########################\n",
    "            ##########################\n",
    "            \n",
    "            ############################################################\n",
    "            #For pure Zhang+20 method, exclude all star forming regions#\n",
    "            ############################################################\n",
    "\n",
    "            pipe_cell_coords_z = pipe_cell_coords[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_dens_z = pipe_cell_dens[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_elab_z = pipe_cell_elab[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_sfr_z = pipe_cell_sfr[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_dark_z = pipe_cell_dark[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_pIDs_z = pipe_cell_pIDs[np.where(pipe_cell_sfr==0)]\n",
    "            pipe_cell_shIDs_z = pipe_cell_shIDs[np.where(pipe_cell_sfr==0)]\n",
    "\n",
    "            #print('sum for star forming check: {0}'.format(pipe_cell_sfr_z.sum()))\n",
    "\n",
    "            #############################################################################################\n",
    "            #For Pakmor+18 method, apply correction to A for star forming regions and leave no cells out#\n",
    "            #############################################################################################\n",
    "\n",
    "            pipe_cell_coords_p = pipe_cell_coords[:]\n",
    "            pipe_cell_dens_p   = pipe_cell_dens[:]\n",
    "            pipe_cell_elab_p   = pipe_cell_elab[:]*pipe_cell_warm[:] #perform Pakmor correction\n",
    "            pipe_cell_sfr_p    = pipe_cell_sfr[:]\n",
    "            pipe_cell_dark_p   = pipe_cell_dark[:]\n",
    "            pipe_cell_pIDs_p   = pipe_cell_pIDs[:]\n",
    "            pipe_cell_shIDs_p  = pipe_cell_shIDs[:]\n",
    "            \n",
    "            #print(pipe_cell_pIDs_p)\n",
    "            #print(pipe_cell_shIDs_p)\n",
    "\n",
    "            ###############################################\n",
    "            #divide pipe into 10,000 bins along the x-axis#\n",
    "            ###############################################\n",
    "\n",
    "            #Question: why 10,000 bins given there are so few particles in the pipe?\n",
    "\n",
    "            pipe_x_bins = np.linspace(pipe_start_coords[0],pipe_end_coords[0],nbins)\n",
    "            #print('Pipe x-axis bin coordinates: {0} ckpc/h'.format(pipe_x_bins))\n",
    "\n",
    "            #######################################\n",
    "            #get coordinates of center of each bin#\n",
    "            #######################################\n",
    "\n",
    "            pipe_bin_coords = np.array([[i,pipe_start_coords[1],pipe_start_coords[2]]for i in pipe_x_bins])\n",
    "\n",
    "\n",
    "            ###############################################################\n",
    "            #for each bin, find distance between it and every cell in pipe#\n",
    "            #find the one with miniimum distance                          #\n",
    "            #this will be the cell in the los                             #\n",
    "            #do for zhang (excluding sfr) and non-zhang (including sfr)   #\n",
    "            ###############################################################\n",
    "\n",
    "\n",
    "            ###########\n",
    "            #Pakmor   #\n",
    "            ###########\n",
    "\n",
    "            #initialise empty array to hold indices of closest particle to each bin\n",
    "            nearest_idxs_p = []\n",
    "\n",
    "            for i in range(len(pipe_bin_coords)): #loop over bins\n",
    "                coords = pipe_bin_coords[i] #get bin coordinates\n",
    "                distarr = np.sqrt(np.sum(((pipe_cell_coords_p[:]-coords)**2),axis=1)) #create array of distances from cells\n",
    "                nearest = np.argmin(distarr) #find nearest cell to bin\n",
    "                nearest_idxs_p.append(nearest) #append to array\n",
    "\n",
    "            nearest_idxs_p = np.array(nearest_idxs_p) #convert to numpy array\n",
    "            nearest_idxs_unique_p = np.unique(nearest_idxs_p) #some cells are the closest to multiple bins. Get uniques.\n",
    "\n",
    "            ##############\n",
    "            #zhang method#\n",
    "            ##############\n",
    "\n",
    "            #initialise empty array to hold indices of closest particle to each bin\n",
    "            nearest_idxs_z = []\n",
    "\n",
    "            for i in range(len(pipe_bin_coords)): #loop over bins\n",
    "                coords = pipe_bin_coords[i] #get bin coordinates\n",
    "                distarr = np.sqrt(np.sum(((pipe_cell_coords_z[:]-coords)**2),axis=1)) #create array of distances from cells\n",
    "                nearest = np.argmin(distarr) #find nearest cell to bin\n",
    "                nearest_idxs_z.append(nearest) #append to array\n",
    "\n",
    "            nearest_idxs_z = np.array(nearest_idxs_z) #convert to numpy array\n",
    "            nearest_idxs_unique_z = np.unique(nearest_idxs_z) #some cells are the closest to multiple bins. Get uniques.\n",
    "\n",
    "            #print('Nearest {0} particle ids: {1}'.format(np.shape(nearest_idxs),nearest_idxs))\n",
    "            #print('Of these, {0} are unique: {1}'.format(np.shape(nearest_idxs_unique),nearest_idxs_unique))\n",
    "\n",
    "            #################################\n",
    "            #extract data from nearest cells#\n",
    "            #################################\n",
    "\n",
    "            ###########\n",
    "            #Pakmor   #\n",
    "            ###########\n",
    "\n",
    "            pipe_nearest_coords_p = np.array(pipe_cell_coords_p[nearest_idxs_p]) #coordinates [ckpc/h]\n",
    "            pipe_nearest_dens_p   = np.array(pipe_cell_dens_p[nearest_idxs_p])   #densities [(1e10Msun/h)/(ckpc/h)**3]\n",
    "            pipe_nearest_elab_p   = np.array(pipe_cell_elab_p[nearest_idxs_p])   #electron abundance [-]\n",
    "            pipe_nearest_sfr_p    = np.array(pipe_cell_sfr_p[nearest_idxs_p])    #star formation rate [Msun/yr]\n",
    "            pipe_nearest_dark_p   = np.array(pipe_cell_dark_p[nearest_idxs_p])   #comoving dark matter density [(1e10Msun/h)/(ckpc/h)**3]\n",
    "            pipe_nearest_pIDs_p   = np.array(pipe_cell_pIDs_p[nearest_idxs_p])   #particle ID numbers\n",
    "            pipe_nearest_shIDs_p  = np.array(pipe_cell_shIDs_p[nearest_idxs_p])  #subhalo ID numbers\n",
    "            \n",
    "            #######\n",
    "            #zhang#\n",
    "            #######\n",
    "            \n",
    "            pipe_nearest_coords_z = np.array(pipe_cell_coords_z[nearest_idxs_z]) #coordinates [ckpc/h]\n",
    "            pipe_nearest_dens_z   = np.array(pipe_cell_dens_z[nearest_idxs_z])   #densities [(1e10Msun/h)/(ckpc/h)**3]\n",
    "            pipe_nearest_elab_z   = np.array(pipe_cell_elab_z[nearest_idxs_z])   #electron abundance [-]\n",
    "            pipe_nearest_sfr_z    = np.array(pipe_cell_sfr_z[nearest_idxs_z])    #star formation rate [Msun/yr]\n",
    "            pipe_nearest_dark_z   = np.array(pipe_cell_dark_z[nearest_idxs_z])   #comoving dark matter density [(1e10Msun/h)/(ckpc/h)**3] \n",
    "            pipe_nearest_pIDs_z   = np.array(pipe_cell_pIDs_z[nearest_idxs_z])   #particle ID numbers\n",
    "            pipe_nearest_shIDs_z  = np.array(pipe_cell_shIDs_z[nearest_idxs_z])  #particle ID numbers\n",
    "            \n",
    "            #############################################\n",
    "            #############################################\n",
    "            ##subhalo central coordinates insert begins##\n",
    "            #############################################\n",
    "            #############################################\n",
    "            \n",
    "            #get first subhalo id\n",
    "            first_shID = pipe_nearest_shIDs_p[0]\n",
    "            \n",
    "            #get unique subhalo ids in the pipe\n",
    "            unique_shIDs = np.unique(pipe_nearest_shIDs_p)\n",
    "            \n",
    "            #get non- negative one subhalos\n",
    "            unique_shIDs_notneg1 = np.where(unique_shIDs!=-1)\n",
    "            unique_shIDs_notneg1 = unique_shIDs[unique_shIDs_notneg1]\n",
    "            \n",
    "            #get central coordinates for subhalos with non -1 subhalo IDs\n",
    "            closest_coords = [] #initialise array to store\n",
    "            \n",
    "            for shID in unique_shIDs_notneg1:\n",
    "                print('shid: {0}, snap numberr: {1}'.format(shID,snap_number))\n",
    "                gas = il.snapshot.loadSubhalo(basePath, snap_number, shID, 'gas', fields=None)\n",
    "                subhalo = il.groupcat.loadSingle(basePath, snap_number, subhaloID=shID)\n",
    "                centralpos = subhalo['SubhaloPos']\n",
    "                print('shid central pos: {0}'.format(centralpos))\n",
    "                \n",
    "                #get coordinates of closest approach to these subhalo IDs\n",
    "                placeholder = np.copy(pipe_bin_coords[0]) #placeholder coordinates, y and z will be equal to sightline\n",
    "                placeholder[0] = centralpos[0]   #set x position equal to that of subhalo center.\n",
    "                print('closest: {0}'.format(placeholder))\n",
    "                closest_coords.append(placeholder)\n",
    "            print('unique subhalo IDs in pipe: {0}'.format(unique_shIDs))\n",
    "            print('placeholder coordinates for point of closest approach: {0}'.format(closest_coords))\n",
    "            \n",
    "            ###########################################\n",
    "            ###########################################\n",
    "            ##subhalo central coordinates insert ends##\n",
    "            ###########################################\n",
    "            ###########################################\n",
    "            \n",
    "            ###############################################\n",
    "            #convert density to si units using artale code#\n",
    "            ###############################################\n",
    "\n",
    "            pipe_nearest_dens_p_si = TNG_Dens2SI_astropy(pipe_nearest_dens_p)\n",
    "            pipe_nearest_dens_z_si = TNG_Dens2SI_astropy(pipe_nearest_dens_z)\n",
    "\n",
    "            ###########################################################\n",
    "            #convert dark matter density to si units using artale code#\n",
    "            ###########################################################\n",
    "\n",
    "            pipe_nearest_dark_p_si = TNG_Dens2SI_astropy(pipe_nearest_dark_p)         \n",
    "            pipe_nearest_dark_z_si = TNG_Dens2SI_astropy(pipe_nearest_dark_z)         \n",
    "\n",
    "            #########################################################################\n",
    "            #divide dark matter density by critical density to create the LSS tracer#\n",
    "            #########################################################################\n",
    "\n",
    "            pipe_nearest_LSStracer_p = pipe_nearest_dark_p_si/my_dens_crit\n",
    "            pipe_nearest_LSStracer_z = pipe_nearest_dark_z_si/my_dens_crit\n",
    "            #print('The structure tracer array is {0}'.format(pipe_nearest_LSStracer_z))       \n",
    "\n",
    "            ##########################################\n",
    "            #Create Large-Scale Structure (LSS) masks#\n",
    "            ##########################################\n",
    "\n",
    "            #non-zhang\n",
    "            voi_mask_PT0_p = pipe_nearest_LSStracer_p < 0.1\n",
    "            fil_mask_PT0_p = np.logical_and(pipe_nearest_LSStracer_p >= 0.1, pipe_nearest_LSStracer_p < 57)#CELESTE:CORRECTED\n",
    "            hal_mask_PT0_p = pipe_nearest_LSStracer_p >= 57 \n",
    "\n",
    "            #zhang\n",
    "            voi_mask_PT0_z = pipe_nearest_LSStracer_z < 0.1\n",
    "            fil_mask_PT0_z = np.logical_and(pipe_nearest_LSStracer_z >= 0.1, pipe_nearest_LSStracer_z < 57)#CELESTE:CORRECTED\n",
    "            hal_mask_PT0_z = pipe_nearest_LSStracer_z >= 57        \n",
    "\n",
    "            ##############################################################\n",
    "            #Calculate the number of nearest cells of each structure type#\n",
    "            ##############################################################\n",
    "\n",
    "            num_voi_cells_z = np.shape(pipe_nearest_coords_z[voi_mask_PT0_z])[0]\n",
    "            num_fil_cells_z = np.shape(pipe_nearest_coords_z[fil_mask_PT0_z])[0]\n",
    "            num_hal_cells_z = np.shape(pipe_nearest_coords_z[hal_mask_PT0_z])[0]\n",
    "\n",
    "            num_voi_cells_p = np.shape(pipe_nearest_coords_p[voi_mask_PT0_p])[0]\n",
    "            num_fil_cells_p = np.shape(pipe_nearest_coords_p[fil_mask_PT0_p])[0]\n",
    "            num_hal_cells_p = np.shape(pipe_nearest_coords_p[hal_mask_PT0_p])[0]\n",
    "\n",
    "            ##########################################\n",
    "            #get electron density at each of the bins#\n",
    "            ##########################################\n",
    "\n",
    "            #follow zhang+20 equation exactly as native units of TNG are\n",
    "            #comoving\n",
    "\n",
    "            #############################################################\n",
    "            #Zhang: pne = (ElAb)*hmasssfrac*(Dens/protonmass)*((1+z)**3)#\n",
    "            #use data which excludes SFRs                               #\n",
    "            #############################################################\n",
    "\n",
    "            #total\n",
    "            pipe_nearest_pne_z = (pipe_nearest_elab_z)*hmassfrac*(pipe_nearest_dens_z_si/protonmass)*((1+header['Redshift'])**3)\n",
    "            pipe_nearest_pne_p = (pipe_nearest_elab_p)*hmassfrac*(pipe_nearest_dens_p_si/protonmass)*((1+header['Redshift'])**3)\n",
    "            #print('pnes are: {0}'.format(pipe_nearest_pne_z))\n",
    "\n",
    "            #halos\n",
    "            pipe_nearest_pne_z_hal = (pipe_nearest_elab_z[hal_mask_PT0_z])*hmassfrac*(pipe_nearest_dens_z_si[hal_mask_PT0_z]/protonmass)*((1+header['Redshift'])**3)\n",
    "            pipe_nearest_pne_p_hal = (pipe_nearest_elab_z[hal_mask_PT0_p])*hmassfrac*(pipe_nearest_dens_p_si[hal_mask_PT0_p]/protonmass)*((1+header['Redshift'])**3)\n",
    "            #print('pnes in halos are: {0}'.format(pipe_nearest_pne_z_hal))\n",
    "\n",
    "            #filaments\n",
    "            pipe_nearest_pne_z_fil = (pipe_nearest_elab_z[fil_mask_PT0_z])*hmassfrac*(pipe_nearest_dens_z_si[fil_mask_PT0_z]/protonmass)*((1+header['Redshift'])**3)\n",
    "            pipe_nearest_pne_p_fil = (pipe_nearest_elab_p[fil_mask_PT0_p])*hmassfrac*(pipe_nearest_dens_p_si[fil_mask_PT0_p]/protonmass)*((1+header['Redshift'])**3)\n",
    "            #print('pnes in filaments are: {0}'.format(pipe_nearest_pne_z_fil))\n",
    "\n",
    "            #voids\n",
    "            pipe_nearest_pne_z_voi = (pipe_nearest_elab_z[voi_mask_PT0_z])*hmassfrac*(pipe_nearest_dens_z_si[voi_mask_PT0_z]/protonmass)*((1+header['Redshift'])**3)\n",
    "            pipe_nearest_pne_p_voi = (pipe_nearest_elab_z[voi_mask_PT0_p])*hmassfrac*(pipe_nearest_dens_p_si[voi_mask_PT0_p]/protonmass)*((1+header['Redshift'])**3)\n",
    "            #print('pnes in voids are: {0}'.format(pipe_nearest_pne_z_voi))\n",
    "\n",
    "\n",
    "            ######################################################################\n",
    "            #Non-zhang: pne = (ElAb*Warm)*hmasssfrac*(Dens/protonmass)*((1+z)**3)#\n",
    "            #use all data (sfr included) and warm mass fraction                  #\n",
    "            ######################################################################    \n",
    "\n",
    "            ##################################\n",
    "            #average these electron densities#\n",
    "            ##################################\n",
    "\n",
    "            #Zhang method/Pakmor method\n",
    "\n",
    "            #total\n",
    "            pipe_average_pne_z = np.mean(pipe_nearest_pne_z)\n",
    "            pipe_average_pne_p = np.mean(pipe_nearest_pne_p)\n",
    "            #print('Average pne is: {0}'.format(pipe_average_pne_z))   \n",
    "\n",
    "            #halos\n",
    "            pipe_average_pne_z_hal = np.sum(pipe_nearest_pne_z_hal)/nbins\n",
    "            pipe_average_pne_p_hal = np.sum(pipe_nearest_pne_p_hal)/nbins\n",
    "            #print('Average pne in halos is: {0}'.format(pipe_average_pne_z_hal))\n",
    "\n",
    "            #filaments\n",
    "            pipe_average_pne_z_fil = np.sum(pipe_nearest_pne_z_fil)/nbins\n",
    "            pipe_average_pne_p_fil = np.sum(pipe_nearest_pne_p_fil)/nbins\n",
    "            #print('Average pne in filaments is: {0}'.format(pipe_average_pne_z_fil))\n",
    "\n",
    "            #voids\n",
    "            pipe_average_pne_z_voi = np.sum(pipe_nearest_pne_z_voi)/nbins\n",
    "            pipe_average_pne_p_voi = np.sum(pipe_nearest_pne_p_voi)/nbins\n",
    "            #print('Average pne in voids is: {0}'.format(pipe_average_pne_z_voi))\n",
    "\n",
    "\n",
    "            ################################\n",
    "            #calculate dDM/dz for this pipe#\n",
    "            ################################\n",
    "\n",
    "            #outer bit of eq 7\n",
    "            outer=c.c/cosmosource.H(0)\n",
    "            #print(outer)\n",
    "\n",
    "            #E(z) according to paper eq 5\n",
    "            Ez = np.sqrt((0.3089*((1+header['Redshift'])**(3)))+(0.6911))\n",
    "            #print(Ez)\n",
    "\n",
    "            #denominator of eq 7\n",
    "            denominator = ((1+header['Redshift'])**(2))*Ez\n",
    "\n",
    "            #remainder of equation 7\n",
    "\n",
    "            #total\n",
    "            edens_z = pipe_average_pne_z\n",
    "            ddmdz_z = outer*edens_z/denominator\n",
    "            edens_p = pipe_average_pne_p\n",
    "            ddmdz_p = outer*edens_p/denominator\n",
    "            #print('dDM/dz = {0}'.format(ddmdz_z.to('pc*cm**(-3)')))\n",
    "\n",
    "            #halos\n",
    "            edens_z_hal = pipe_average_pne_z_hal\n",
    "            ddmdz_z_hal = outer*edens_z_hal/denominator\n",
    "            edens_p_hal = pipe_average_pne_p_hal\n",
    "            ddmdz_p_hal = outer*edens_p_hal/denominator\n",
    "\n",
    "            #filaments\n",
    "            edens_z_fil = pipe_average_pne_z_fil\n",
    "            ddmdz_z_fil = outer*edens_z_fil/denominator\n",
    "            edens_p_fil = pipe_average_pne_p_fil\n",
    "            ddmdz_p_fil = outer*edens_p_fil/denominator\n",
    "\n",
    "            #voids\n",
    "            edens_z_voi = pipe_average_pne_z_voi\n",
    "            ddmdz_z_voi = outer*edens_z_voi/denominator\n",
    "            edens_p_voi = pipe_average_pne_p_voi\n",
    "            ddmdz_p_voi = outer*edens_p_voi/denominator\n",
    "\n",
    "            ################################\n",
    "            #append new data to dictionary #\n",
    "            ################################\n",
    "\n",
    "            dict_to_edit['dDMdz_Zhang'].append(ddmdz_z.to('pc*cm**(-3)').value) #append total dDM/dz to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzHalo_Zhang'].append(ddmdz_z_hal.to('pc*cm**(-3)').value) #append Halo value to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzFilament_Zhang'].append(ddmdz_z_fil.to('pc*cm**(-3)').value) #append Filament value to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzVoid_Zhang'].append(ddmdz_z_voi.to('pc*cm**(-3)').value) #append Void value to array in [pc/cc]\n",
    "            dict_to_edit['nHalo_Zhang'].append(num_hal_cells_z) #append number of cells in halos used to get this dDM/dz value to array\n",
    "            dict_to_edit['nFilament_Zhang'].append(num_fil_cells_z) #append number of cells in filaments used to get this dDM/dz value to array\n",
    "            dict_to_edit['nVoid_Zhang'].append(num_voi_cells_z) #append number of cells in voids used to get this dDM/dz value to array\n",
    "\n",
    "            dict_to_edit['dDMdz_Pakmor'].append(ddmdz_p.to('pc*cm**(-3)').value) #append total dDM/dz to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzHalo_Pakmor'].append(ddmdz_p_hal.to('pc*cm**(-3)').value) #append Halo value to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzFilament_Pakmor'].append(ddmdz_p_fil.to('pc*cm**(-3)').value) #append Filament value to array in [pc/cc]\n",
    "            dict_to_edit['dDMdzVoid_Pakmor'].append(ddmdz_p_voi.to('pc*cm**(-3)').value) #append Void value to array in [pc/cc]\n",
    "            dict_to_edit['nHalo_Pakmor'].append(num_hal_cells_p) #append number of cells in halos used to get this dDM/dz value to array\n",
    "            dict_to_edit['nFilament_Pakmor'].append(num_fil_cells_p) #append number of cells in filaments used to get this dDM/dz value to array\n",
    "            dict_to_edit['nVoid_Pakmor'].append(num_voi_cells_p) #append number of cells in voids used to get this dDM/dz value to array\n",
    "\n",
    "            #edit 09/02/22 for storing information about subhalos along sightline\n",
    "            dict_to_edit['firstShID'].append(first_shID) #append first subhalo ID number along pipe line of sight\n",
    "            dict_to_edit['uniqueShIDs'].append(unique_shIDs) #append unique subhalo ID numbers along pipe line of sight\n",
    "            dict_to_edit['closestCoords'].append(closest_coords) #append closest coordinates along pipe line of sight to these subhalos\n",
    "        \n",
    "            #########################\n",
    "            #save updated dictionary#\n",
    "            #########################\n",
    "            np.save('{0}'.format(outfile_name),dict_to_edit)\n",
    "\n",
    "            ###########################\n",
    "            #reload updated dictionary#\n",
    "            ###########################\n",
    "            dict_to_edit = np.load(outfile_name,allow_pickle=True).tolist()\n",
    "            print('New length = {0}'.format(len(dict_to_edit['dDMdz_Pakmor'])))\n",
    "\n",
    "        ###############################\n",
    "        ##Once snapshot is done, print#\n",
    "        ###############################\n",
    "\n",
    "        print('Completed and stored {0}\\n with {1} keys of length {2}\\n'.format(outfile_name,len(dict_to_edit),len(dict_to_edit['dDMdz_Pakmor'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
