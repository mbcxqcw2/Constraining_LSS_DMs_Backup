{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41517649",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925d684",
   "metadata": {},
   "source": [
    "This notebook creates subhalo ID and particle ID lists for snapshots for IllustrisTNG simulations.\n",
    "\n",
    "These can be loaded and used for cross-matching for impact factors to galaxies.\n",
    "\n",
    "The code is updated from: \n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/Create_Particle_Subhalo_Matchlists_2.ipynb\n",
    "\n",
    "using:\n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/test_artale_vs_yt_field_load.ipynb\n",
    "\n",
    "- It builds on previous work by creating lists for any desired simulation.\n",
    "- It uses illustris_python instead of yt to load simulation particle ID list.\n",
    "- It loads particle ID data and creates subhalo IDs in chunks, and stores the matchlists in chunks to accomodate larger simulations (e.g. TNG300-1)\n",
    "- As output for larger simulations is huge, it now stores matchlists to /ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/\n",
    "- It also paralellises this code over multiple CPUs and tests the results. They seem to match.\n",
    "\n",
    "    #NOTE by charlie on 10/03/22: in the code, I feed to_ID into pID2shID(). I think this is correct.\n",
    "    #this would mean I am feeding it the INDICES of the particle IDs, not the particle IDs themselves.\n",
    "    #this would mean I need to update the documentation of this function.\n",
    "    #this is tested below by feeding it AllPartList instead of to_ID, which always returns -1.\n",
    "    \n",
    "    #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend \n",
    "    #the loading in the pipe creation code to parse through each part separately.\n",
    "    \n",
    "    #NOTE: If you run both the chunked and non-chunked versions of this code for a single snapshot\n",
    "    #for a small simulation, you can perform the test and see the output is the same.\n",
    "    \n",
    "    #NOTE 17/03/22: sometimes if you use too many cores the multiprocessing version hangs after ccompletion.\n",
    "    #Don't know how to fix this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af7fcd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as m\n",
    "import illustris_python as il\n",
    "\n",
    "from contextlib import closing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from charlie_TNG_tools import pID2shID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5edba",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation to create lists for\n",
    "sim_to_use = 'TNG50-4'\n",
    "\n",
    "#snapshots to create lists for\n",
    "snap_list = [40]#[99,91,84,78,72,67,59,50,40,33,25,21,17]\n",
    "\n",
    "#base path to data\n",
    "basePath = '/virgo/simulations/IllustrisTNG/{0}/output/'.format(sim_to_use)\n",
    "\n",
    "#path to simulation hdf5 file\n",
    "simPath = '/virgo/simulations/IllustrisTNG/{0}/simulation.hdf5'.format(sim_to_use)\n",
    "\n",
    "#check these exist\n",
    "print('Testing whether basePath and simPath exist...')\n",
    "print('basePath exists = {0}'.format(os.path.exists(basePath)))\n",
    "print('simPath exists = {0},\\n{1}'.format(os.path.exists(simPath),simPath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f18c5",
   "metadata": {},
   "source": [
    "# Create directories to store matchlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4476589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if a top directory exists\n",
    "#topdir_name = '/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/'\n",
    "topdir_name = '/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/' #edit 14/03/2022: put in /ptmp/ now due to size for large simulations\n",
    "topdir_check = os.path.exists(topdir_name)\n",
    "\n",
    "#if it exists, print\n",
    "if topdir_check == True:\n",
    "    print('Top directory ({0}) already exists.'.format(topdir_name))\n",
    "    \n",
    "#else, create it.\n",
    "elif topdir_check == False:\n",
    "    print('Creating top directory for matchlists at {0}...'.format(topdir_name))\n",
    "    os.mkdir(topdir_name)\n",
    "    print('{0} created.'.format(topdir_name))\n",
    "    \n",
    "    \n",
    "#check to see if subdirectory for particular simulation matchlist exists\n",
    "subdir_name = topdir_name+'Matchlist_dir_{0}'.format(sim_to_use)\n",
    "subdir_check = os.path.exists(subdir_name)\n",
    "\n",
    "#if it exists, print\n",
    "if subdir_check == True:\n",
    "    print('Directory to hold {0} matchlist ({1} exists.)'.format(sim_to_use,subdir_name))\n",
    "\n",
    "#else, create it\n",
    "elif subdir_check == False:\n",
    "    print('Creating subdirectory {0}...'.format(subdir_name))\n",
    "    os.mkdir(subdir_name)\n",
    "    print('{0} created.'.format(subdir_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139b80a",
   "metadata": {},
   "source": [
    "# Create desired matchlists -- version that works on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over snapshots\n",
    "for snapshot_number in snap_list:\n",
    "    print('Creating for {0} snap {1}'.format(sim_to_use,snapshot_number))\n",
    "    \n",
    "    uniquestest_notchunked = [] #test array to record all unique subhalo IDs to test against the non-chunked version\n",
    "\n",
    "\n",
    "     #########################\n",
    "     #SNAPSHOT DATA LOCATIONS#\n",
    "     #########################\n",
    "\n",
    "    offsetFile = '/virgo/simulations/IllustrisTNG/{0}/postprocessing/offsets/offsets_0{1}.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    data_loc = '/virgo/simulations/IllustrisTNG/{0}/output/snapdir_0{1}/snap_0{1}.0.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    partIDs_loc = '/Snapshots/{0}/PartType0/ParticleIDs'.format(snapshot_number)\n",
    "    \n",
    "    print('Processing snapshot {0} at:\\n{1}\\n with offset file:\\n{2}\\n and particle IDs file loc:\\n{3}'.format(snapshot_number,data_loc,offsetFile,partIDs_loc))\n",
    "\n",
    "    #########################\n",
    "    #CREATE PARTICLE ID LIST#\n",
    "    #########################\n",
    "    \n",
    "    #get all gas particle IDs in snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "        allparts = f[partIDs_loc][:]\n",
    "    \n",
    "    #create a list version of every particle ID\n",
    "    AllPartList = allparts.tolist()\n",
    "    \n",
    "\n",
    "    \n",
    "    ########################\n",
    "    #CREATE SUBHALO ID LIST#\n",
    "    ########################\n",
    "    \n",
    "    #load and index cell data\n",
    "    #ds=yt.load(data_loc)\n",
    "    #ds.index\n",
    "    \n",
    "    #choose what to ID\n",
    "    #to_ID = np.arange(ds.particle_type_counts['PartType0'])\n",
    "    to_ID = np.arange(il.snapshot.loadSubset(basePath,snapshot_number,'gas','Density').shape[0])\n",
    "    \n",
    "    #print(np.amin(allparts),\n",
    "    #      np.amax(allparts),\n",
    "    #      np.amax(allparts)-np.amin(allparts),\n",
    "    #     il.snapshot.loadSubset(basePath,snapshot_number,'gas','Density').shape[0])\n",
    "    \n",
    "    partType = 0 #gas\n",
    "    \n",
    "    #choose subhalo fields to load\n",
    "    subhaloFields = ['SubhaloFlag',\n",
    "                     'SubhaloPos',\n",
    "                     'SubhaloHalfmassRad',\n",
    "                     'SubhaloHalfmassRadType',\n",
    "                     'SubhaloLenType']\n",
    "    \n",
    "    #load subhalo catalog\n",
    "    subhalos=il.groupcat.loadSubhalos(basePath,snapshot_number,fields=subhaloFields)  \n",
    "    subhalos.keys()\n",
    "    \n",
    "    #get subhalo offset file for matching particle and subhalo IDs\n",
    "    with h5py.File(offsetFile,'r') as f:\n",
    "        SnapOffsetsSubhalo= np.copy(f['/Subhalo/SnapByType'])\n",
    "    \n",
    "    #get subhalo lengths for all gas particles\n",
    "    SubhaloLenType = np.copy(subhalos['SubhaloLenType'])\n",
    "    \n",
    "    #create array of subhaloIDs for every gas particle \n",
    "    \n",
    "    #note by charlie on 10/03/22 here I feed to_ID into pID2shID(), and I think this is correct.\n",
    "    #this would mean I am feeding it the INDICES of the particle IDs, not the particle IDs themselves.\n",
    "    #this would mean I need to update the documentation of this function.\n",
    "    #this is tested below by feeding it AllPartList instead of to_ID, which always returns -1.\n",
    "    \n",
    "    #print(np.unique(pID2shID(AllPartList,partType,SubhaloLenType,SnapOffsetsSubhalo)),np.unique(AllShIDList))\n",
    "    \n",
    "    AllShIDList = pID2shID(to_ID,partType,SubhaloLenType,SnapOffsetsSubhalo)\n",
    "    print(np.amin(AllShIDList[0:185775]),np.amax(AllShIDList[0:185775]))\n",
    "\n",
    "\n",
    "    uniquestest_notchunked=np.unique(AllShIDList).tolist() #this must be compared to the chunked version for a snapshot\n",
    "\n",
    "    \n",
    "    #####################\n",
    "    #SAVE LISTS TO FILES#\n",
    "    #####################\n",
    "    \n",
    "    np.save('/{0}/PartList_Snap{1}.npy'.format(subdir_name,snapshot_number),AllPartList)\n",
    "    np.save('/{0}/ShIDList_Snap{1}.npy'.format(subdir_name,snapshot_number),AllShIDList)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da012942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(uniquestest_notchunked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158733e5",
   "metadata": {},
   "source": [
    "# Create desired matchlists -- version which chunks for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aff93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks=1000 #the number of chunks (minus remainder) to process the particle data in\n",
    "\n",
    "\n",
    "#loop over snapshots\n",
    "for snapshot_number in snap_list:\n",
    "    \n",
    "    uniquestest_chunked = [] #test array to record all unique subhalo IDs to test against the non-chunked version\n",
    "    \n",
    "    print('Creating for {0} snap {1}'.format(sim_to_use,snapshot_number))\n",
    "    \n",
    "     #########################\n",
    "     #SNAPSHOT DATA LOCATIONS#\n",
    "     #########################\n",
    "\n",
    "    offsetFile = '/virgo/simulations/IllustrisTNG/{0}/postprocessing/offsets/offsets_0{1}.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    data_loc = '/virgo/simulations/IllustrisTNG/{0}/output/snapdir_0{1}/snap_0{1}.0.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    partIDs_loc = '/Snapshots/{0}/PartType0/ParticleIDs'.format(snapshot_number)\n",
    "    \n",
    "    print('Processing snapshot {0} at:\\n{1}\\n with offset file:\\n{2}\\n and particle IDs file loc:\\n{3}'.format(snapshot_number,data_loc,offsetFile,partIDs_loc))\n",
    "\n",
    "    \n",
    "    #########################\n",
    "    #CREATE PARTICLE ID LIST#\n",
    "    #########################\n",
    "    \n",
    "    \n",
    "    #get all gas particle IDs in snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "        \n",
    "        \n",
    "        #you can load smaller chunks of this at a time! Exploit this.\n",
    "        \n",
    "        \n",
    "        simparts = f[partIDs_loc].shape[0] #get the total number of particles in the simulation\n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        #chunking information#\n",
    "        ######################\n",
    "                \n",
    "        print('calculating chunking information for {0} particles over {1} desired chunks...'.format(simparts,\n",
    "                                                                                                     nchunks))\n",
    "        \n",
    "        nchunks_whole = nchunks #number of whole chunks to process\n",
    "        parts_per_chunk = simparts//nchunks #number of particles in a whole chunk which will be processed\n",
    "        remainder = simparts%nchunks #number of particles in the remainder once whole chunks are processed.\n",
    "        if remainder!=0: #if there is any remainder...\n",
    "            nchunks_partial = 1 #one extra chunk will be processed, which is not the same size as the others\n",
    "        else: #if there is no remainder...\n",
    "            nchunks_partial = 0 #no extra chunks will be processed.\n",
    "        nchunks_total = nchunks_whole + nchunks_partial #the total number of chunks (plus any remainder) to process\n",
    "        \n",
    "        print('will process simulation in {0} chunks. {1} chunks of {2} particles, and {3} chunks of {4}.'.format(nchunks_total,\n",
    "                                                                                                                  nchunks_whole,\n",
    "                                                                                                                  parts_per_chunk,\n",
    "                                                                                                                  nchunks_partial,\n",
    "                                                                                                                  remainder))\n",
    "        ###################################\n",
    "        #indices of chunks (and remainder)#\n",
    "        ###################################\n",
    "        \n",
    "        #calculate the particle indices to load for each whole chunk\n",
    "        indices = [[i*parts_per_chunk,(i*parts_per_chunk)+(parts_per_chunk-1)] for i in range(nchunks_whole)]\n",
    "        #append the particle indices for the remainder chunk\n",
    "        indices.append([nchunks*(parts_per_chunk),(nchunks*parts_per_chunk)-1+(remainder)])\n",
    "        print('particles will be loaded according to: {0}'.format(indices))\n",
    "\n",
    "              \n",
    "        ########################################\n",
    "        #loop and load the particles to process#\n",
    "        ########################################\n",
    "        \n",
    "        for chunk_id in range(nchunks_total):\n",
    "            print('loading chunk {0}'.format(chunk_id))\n",
    "            chunk_first_idx = indices[chunk_id][0]\n",
    "            chunk_last_idx = indices[chunk_id][-1]\n",
    "            print('chunk indices: ({0},{1})'.format(chunk_first_idx,chunk_last_idx))\n",
    "\n",
    "            \n",
    "        \n",
    "            #allparts = f[partIDs_loc][:] #do with all particles in simulation\n",
    "            #allparts = f[partIDs_loc][0:5] #do with subsection of particles\n",
    "            #print('a',[partIDs_loc],chunk_first_idx,chunk_last_idx)\n",
    "            #print('b',f[partIDs_loc][chunk_first_idx:chunk_last_idx])\n",
    "            #print('c',f[partIDs_loc])\n",
    "            allparts = np.copy(f[partIDs_loc][chunk_first_idx:chunk_last_idx]) #load the particles in this chunk to ID\n",
    "            #allparts = np.copy(f[partIDs_loc][0:5])\n",
    "            nparts = allparts.shape[0] #get number of particles in this chunk\n",
    "            #print('d',allparts,nparts)\n",
    "            print('chunk loaded.')\n",
    "\n",
    "            #create a list version of every particle ID in this chunk\n",
    "            AllPartList = allparts.tolist()\n",
    "       \n",
    "       \n",
    "            ########################\n",
    "            #CREATE SUBHALO ID LIST#\n",
    "            ########################\n",
    "   \n",
    "            #load and index cell data\n",
    "            #ds=yt.load(data_loc)\n",
    "            #ds.index\n",
    "   \n",
    "            #choose which particles to ID\n",
    "            #to_ID = np.arange(ds.particle_type_counts['PartType0'])\n",
    "            #to_ID = np.arange(il.snapshot.loadSubset(basePath,snapshot_number,'gas','Density').shape[0])\n",
    "            #to_ID = np.arange(nparts) #replace with just the particles we load\n",
    "            to_ID = np.arange(chunk_first_idx,chunk_last_idx+1) #replace with the indices we want to load\n",
    "\n",
    "            #print(to_ID) #we'll have to replace the yt load with a chunked version\n",
    "    \n",
    "    \n",
    "            partType = 0 #gas\n",
    "    \n",
    "            #choose subhalo fields to load\n",
    "            subhaloFields = ['SubhaloFlag',\n",
    "                             'SubhaloPos',\n",
    "                             'SubhaloHalfmassRad',\n",
    "                             'SubhaloHalfmassRadType',\n",
    "                             'SubhaloLenType']\n",
    "    \n",
    "            #load subhalo catalog\n",
    "            subhalos=il.groupcat.loadSubhalos(basePath,snapshot_number,fields=subhaloFields) \n",
    "            subhalos.keys()\n",
    "    \n",
    "            #print(subhalos)\n",
    "\n",
    "    \n",
    "            #get subhalo offset file for matching particle and subhalo IDs\n",
    "            with h5py.File(offsetFile,'r') as f2:\n",
    "                SnapOffsetsSubhalo= np.copy(f2['/Subhalo/SnapByType'])\n",
    "    \n",
    "            #get subhalo lengths for all gas particles\n",
    "            SubhaloLenType = np.copy(subhalos['SubhaloLenType'])\n",
    "    \n",
    "            #create array of subhaloIDs for every gas particle\n",
    "            AllShIDList = pID2shID(to_ID,partType,SubhaloLenType,SnapOffsetsSubhalo)\n",
    "    \n",
    "            #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend the loading\n",
    "            #in the pipe creation code to parse through each part separately.\n",
    "    \n",
    "#             print(AllPartList)\n",
    "            #if(chunk_id==0):\n",
    "            #print(np.amin(AllShIDList),np.amax(AllShIDList),np.unique(AllShIDList))\n",
    "            uniquestest_chunked.append(np.unique(AllShIDList).tolist())\n",
    "\n",
    "            np.save('/{0}/PartList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllPartList)\n",
    "            np.save('/{0}/ShIDList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllShIDList)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02512142",
   "metadata": {},
   "source": [
    "# Test to perform on a small simulation to check output is the same for both versions of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea702d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_test_output = np.unique([item for sublist in uniquestest_chunked for item in sublist]).tolist()\n",
    "\n",
    "print(len(chunked_test_output))\n",
    "\n",
    "print('Is chunked version equal to non-chunked version? :{0}'.format(chunked_test_output==uniquestest_notchunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:04d}'.format(72))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362bbc0",
   "metadata": {},
   "source": [
    "# Version of code which processes chunks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06210eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nchunks=1000 #the desied number of chunks (minus remainder) to process the particle data in\n",
    "\n",
    "#identify number of available cores on the system\n",
    "ncpus = m.cpu_count()\n",
    "print('Maximum of {0} cores available to use.'.format(ncpus))\n",
    "\n",
    "cpus_to_use = 8\n",
    "print('Desired number of cores to use: {0}'.format(cpus_to_use))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_matchlist_package(package):\n",
    "    \"\"\"\n",
    "    Helper function for making simulation matchlists in parallel using multiprocessing.\n",
    "    Unpacks the set of data necessary for parsing the simulation and making matchlists\n",
    "    for certain chunks. Then makes those chunks with process_matchlist_chunk().\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    package : a list containing the inpyt data, which are X arguments in the following order:\n",
    "    \n",
    "        nchunks_total   : [int] the total number of chunks the simulation will be split into to create\n",
    "                        matchlist covering the entire simulation\n",
    "        chunkIDs        : [array of ints] the id numbers of the chunks of simulation to be processed\n",
    "                        on this cpu\n",
    "        indices         : [array] array of particle indices to load for each whole chunk. shape of \n",
    "                        (nchunks_total,2)\n",
    "        simPath         : [str] location of sim file to be loaded as hdf5 File containing all the particle IDs\n",
    "                        subhalo IDs will be created for\n",
    "        partIDs_loc     : [str] location of the (partType0) particle IDs for this simulation snapshot\n",
    "        basePath        : [str] base path to the simulation data\n",
    "        snapshot_number : [int] the snapshot of the simulation to process\n",
    "        offsetFile      : [str] location of the offset file for this simulation and snapshot\n",
    "        subdir_name     : [str] location of subdirectory to store output matchlist chunk in\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    output of process_matchlist_chunk()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    verbose=True\n",
    "    \n",
    "    #unwrap the package to feed to process_matchlist_chunk()\n",
    "    nchunks_total   = package[0]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: nchunks_total = {0}'.format(nchunks_total))\n",
    "    chunkIDs        = package[1]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: chunkIDs = {0}'.format(chunkIDs))\n",
    "    indices         = package[2]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: indices = {0}'.format(indices))\n",
    "    simPath         = package[3]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: simPath = {0}'.format(simPath))\n",
    "    partIDs_loc     = package[4]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: partIDs_loc = {0}'.format(partIDs_loc))\n",
    "    basePath        = package[5]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: basePath = {0}'.format(basePath))\n",
    "    snapshot_number = package[6]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: snapshot_number = {0}'.format(snapshot_number))\n",
    "    offsetFile      = package[7]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: offsetFile = {0}'.format(offsetFile))\n",
    "    subdir_name     = package[8]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: subdir_name = {0}'.format(subdir_name))\n",
    "    \n",
    "    #run process_matchlist_chunk()\n",
    "    process_matchlist_chunk(nchunks_total,chunkIDs,indices,simPath,partIDs_loc,basePath,snapshot_number,offsetFile,subdir_name)\n",
    "    \n",
    "    return 'done'\n",
    "\n",
    "def process_matchlist_chunk(nchunks_total,\n",
    "                            chunkIDs,\n",
    "                            indices,\n",
    "                            simPath,\n",
    "                            partIDs_loc,\n",
    "                            basePath,\n",
    "                            snapshot_number,\n",
    "                            offsetFile,\n",
    "                            subdir_name):\n",
    "    \"\"\"\n",
    "\n",
    "    For a simulation snapshot split into nchunks chunks,  creates certain chunks of the simulation \n",
    "    particle/subhalo ID matchlist on a single cpu. It is fed by unwrap_matchlist_package().\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    \n",
    "    nchunks_total   : [int] the total number of chunks the simulation will be split into to create\n",
    "                    matchlist covering the entire simulation\n",
    "    chunkIDs        : [array of ints] the id numbers of the chunks of simulation to be processed\n",
    "                    on this cpu\n",
    "    indices         : [array] array of particle indices to load for each whole chunk. shape of \n",
    "                    (nchunks_total,2)\n",
    "    simPath         : [str] location of sim file to be loaded as hdf5 File containing all the particle IDs  \n",
    "                    subhalo IDs will be created for\n",
    "    partIDs_loc     : [str] location of the (partType0) particle IDs for this simulation snapshot\n",
    "    basePath        : [str] base path to the simulation data\n",
    "    snapshot_number : [int] the snapshot of the simulation to process\n",
    "    offsetFile      : [str] location of the offset file for this simulation and snapshot\n",
    "    subdir_name     : [str] location of subdirectory to store output matchlist chunk in\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    1 (must return something so multiprocessing is guaranteed to terminate after process is finished)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #print(simPath)\n",
    "\n",
    "    #get all gas particle IDs in the simulation snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "\n",
    "        print(f)\n",
    "        \n",
    "        #for the chunks of the simulation to be processed by this CPU...\n",
    "        for i in range(len(chunkIDs)):\n",
    "            \n",
    "            ########################################\n",
    "            #CREATE THE CHUNK PARTICLE ID MATCHLIST#\n",
    "            ########################################\n",
    "            \n",
    "            #select the chunk ID to be processed\n",
    "            chunk_id = chunkIDs[i] \n",
    "\n",
    "            #extract the indices within this chunk which will be loaded\n",
    "            print('loading chunk {0}'.format(chunk_id))\n",
    "            chunk_first_idx = indices[chunk_id][0]\n",
    "            chunk_last_idx = indices[chunk_id][-1]\n",
    "            print('chunk indices: ({0},{1})'.format(chunk_first_idx,chunk_last_idx))\n",
    "\n",
    "            #load only the particles in this chunk, according to the indices\n",
    "            allparts = np.copy(f[partIDs_loc][chunk_first_idx:chunk_last_idx])\n",
    "            \n",
    "            #get number of particles in this chunk\n",
    "            nparts = allparts.shape[0] \n",
    "            print('chunk loaded.')\n",
    "\n",
    "            #create a list version of every particle ID in this chunk\n",
    "            AllPartList = allparts.tolist()\n",
    "            print('Particle list created')\n",
    "\n",
    "            #######################################\n",
    "            #CREATE THE CHUNK SUBHALO ID MATCHLIST#\n",
    "            #######################################\n",
    "\n",
    "            #choose which particles to ID\n",
    "            to_ID = np.arange(chunk_first_idx,chunk_last_idx+1) #replace with the indices we want to load    \n",
    "\n",
    "            #choose the particle type (note: 0 is gas, and it should always be gas, but perhaps this)\n",
    "            #should not be hard coded...\n",
    "            partType = 0 #gas\n",
    "\n",
    "            #choose subhalo fields to load\n",
    "            subhaloFields = ['SubhaloFlag',\n",
    "                             'SubhaloPos',\n",
    "                             'SubhaloHalfmassRad',\n",
    "                             'SubhaloHalfmassRadType',\n",
    "                             'SubhaloLenType']\n",
    "\n",
    "            #load subhalo catalog\n",
    "            subhalos=il.groupcat.loadSubhalos(basePath,snapshot_number,fields=subhaloFields) \n",
    "            subhalos.keys()\n",
    "\n",
    "            #get subhalo offset file for matching particle and subhalo IDs\n",
    "            with h5py.File(offsetFile,'r') as f2:\n",
    "                SnapOffsetsSubhalo= np.copy(f2['/Subhalo/SnapByType'])\n",
    "            print('offsetfile loaded')\n",
    "\n",
    "            #get subhalo lengths for all gas particles\n",
    "            SubhaloLenType = np.copy(subhalos['SubhaloLenType'])\n",
    "\n",
    "            #create array of subhaloIDs for every gas particle\n",
    "            AllShIDList = pID2shID(to_ID,partType,SubhaloLenType,SnapOffsetsSubhalo)\n",
    "            print('ShIDList created')\n",
    "\n",
    "            #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend the loading\n",
    "            #in the pipe creation code to parse through each part separately.\n",
    "            #uniquestest_chunked.append(np.unique(AllShIDList).tolist())\n",
    "            \n",
    "            ############################\n",
    "            #SAVE BOTH MATCHLIST CHUNKS#\n",
    "            ############################\n",
    "            \n",
    "            np.save('/{0}/PartList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllPartList)\n",
    "            np.save('/{0}/ShIDList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllShIDList)  \n",
    "            print('saved')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a5179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nchunks=1000 #the number of chunks (minus remainder) to process the particle data in\n",
    "\n",
    "\n",
    "#loop over snapshots\n",
    "for snapshot_number in snap_list:\n",
    "    \n",
    "    uniquestest_chunked = [] #test array to record all unique subhalo IDs to test against the non-chunked version\n",
    "    \n",
    "    print('Creating for {0} snap {1}'.format(sim_to_use,snapshot_number))\n",
    "    \n",
    "     #########################\n",
    "     #SNAPSHOT DATA LOCATIONS#\n",
    "     #########################\n",
    "\n",
    "    offsetFile = '/virgo/simulations/IllustrisTNG/{0}/postprocessing/offsets/offsets_0{1}.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    data_loc = '/virgo/simulations/IllustrisTNG/{0}/output/snapdir_0{1}/snap_0{1}.0.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    partIDs_loc = '/Snapshots/{0}/PartType0/ParticleIDs'.format(snapshot_number)\n",
    "    \n",
    "    print('Processing snapshot {0} at:\\n{1}\\n with offset file:\\n{2}\\n and particle IDs file loc:\\n{3}'.format(snapshot_number,data_loc,offsetFile,partIDs_loc))\n",
    "\n",
    "    \n",
    "    #########################\n",
    "    #CREATE PARTICLE ID LIST#\n",
    "    #########################\n",
    "    \n",
    "    \n",
    "    #get all gas particle IDs in snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "        \n",
    "        \n",
    "        #you can load smaller chunks of this at a time! Exploit this.\n",
    "        \n",
    "        \n",
    "        simparts = f[partIDs_loc].shape[0] #get the total number of particles in the simulation\n",
    "        \n",
    "        ########################\n",
    "        ########################\n",
    "        ##CHUNKING INFORMATION##\n",
    "        ########################\n",
    "        ########################\n",
    "        \n",
    "        #######################################################################################\n",
    "        #the number of chunks of data to load the particle IDs in according to desired nchunks#\n",
    "        #######################################################################################\n",
    "        \n",
    "        print('calculating chunking information for {0} particles over {1} desired chunks...'.format(simparts,\n",
    "                                                                                                     nchunks))\n",
    "        \n",
    "        nchunks_whole = nchunks #number of whole chunks to process\n",
    "        parts_per_chunk = simparts//nchunks #number of particles in a whole chunk which will be processed\n",
    "        remainder = simparts%nchunks #number of particles in the remainder once whole chunks are processed.\n",
    "        if remainder!=0: #if there is any remainder...\n",
    "            nchunks_partial = 1 #one extra chunk will be processed, which is not the same size as the others\n",
    "        else: #if there is no remainder...\n",
    "            nchunks_partial = 0 #no extra chunks will be processed.\n",
    "        nchunks_total = nchunks_whole + nchunks_partial #the total number of chunks (plus any remainder) to process\n",
    "        \n",
    "        print('will process simulation in {0} chunks. {1} chunks of {2} particles, and {3} chunks of {4}.'.format(nchunks_total,\n",
    "                                                                                                                 nchunks_whole,\n",
    "                                                                                                                  parts_per_chunk,\n",
    "                                                                                                                  nchunks_partial,\n",
    "                                                                                                                  remainder))\n",
    "        ###############################################################################                                                                                                    \n",
    "        #how this loading will be distributed over desired number of cores cpus_to_use#  \n",
    "        ###############################################################################\n",
    "\n",
    "        #choose the number of cores to use at once. \n",
    "        cpus_to_use = cpus_to_use \n",
    "\n",
    "        #calculate the number of full core runs to be used to check for simulation cells in pipe\n",
    "        #this number is the number of parts of the simulation which will be loaded simultaneously\n",
    "        n_full_core = nchunks_total//cpus_to_use\n",
    "        print(n_full_core)\n",
    "        \n",
    "\n",
    "        #calculate the number of cores which must be used to check the remaining simulation cells\n",
    "        #this number is the number of leftover parts of the simulation which will be loaded all at once\n",
    "        n_partial_core = nchunks_total%cpus_to_use\n",
    "\n",
    "        print('To parse these {0} chunks over {1} cpus, {1} cpus will load data simultaneously.\\n\\\n",
    "              This will happen {2} times. The remaining data needs {3} cpus.\\n\\\n",
    "              These will be loaded simultaneously.'.format(nchunks_total,\n",
    "                                                           cpus_to_use,\n",
    "                                                           n_full_core,\n",
    "                                                           n_partial_core))\n",
    "        \n",
    "        ###################################\n",
    "        #indices of chunks (and remainder)#\n",
    "        ###################################\n",
    "        \n",
    "        #calculate the particle indices to load for each whole chunk\n",
    "        indices = [[i*parts_per_chunk,(i*parts_per_chunk)+(parts_per_chunk-1)] for i in range(nchunks_whole)]\n",
    "        #append the particle indices for the remainder chunk\n",
    "        indices.append([nchunks*(parts_per_chunk),(nchunks*parts_per_chunk)-1+(remainder)])\n",
    "        indices=np.array(indices)\n",
    "        print('particles will be loaded according to indices array of shape {0}:\\n\\\n",
    "        {1}'.format(indices.shape,\n",
    "                    indices))\n",
    "        \n",
    "    ###################                   \n",
    "    ###################                                                                                                          \n",
    "    ##PROCESSING DATA##                                                                                                          \n",
    "    ###################\n",
    "    ###################\n",
    "\n",
    "    if n_partial_core == 0: #if there is no remainder to process after full core runs\n",
    "\n",
    "        print('No partial cores to process')\n",
    "\n",
    "        ####################################################\n",
    "        #map of which indices will be processed on each cpu#\n",
    "        ####################################################\n",
    "\n",
    "        cpu_map = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core) \n",
    "        print('CPU map: {0}'.format(cpu_map))\n",
    "\n",
    "        ############################################################################\n",
    "        #package of data to be passed to an unwrapper to unwrap for multiprocessing#\n",
    "        ############################################################################\n",
    "\n",
    "        package = [(nchunks_total,\n",
    "                    cpu_map[i],\n",
    "                    indices,\n",
    "                    simPath,\n",
    "                    partIDs_loc,\n",
    "                    basePath,\n",
    "                    snapshot_number,\n",
    "                    offsetFile,\n",
    "                    subdir_name) for i in range(cpus_to_use)]\n",
    "        #print('package: {0}'.format(package))\n",
    "\n",
    "        #########################\n",
    "        #the multiprocessing bit#\n",
    "        #########################\n",
    "\n",
    "        with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #eeven more terminating\n",
    "\n",
    "    elif n_partial_core > 0: #else if there is remainder to process after full core runs\n",
    "\n",
    "        print('Partial cores to process')\n",
    "\n",
    "        #####################################################\n",
    "        #maps of which indices will be processed on each cpu#\n",
    "        #####################################################\n",
    "\n",
    "        cpu_map_a = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core)\n",
    "        cpu_map_b = np.arange(n_full_core*cpus_to_use,nchunks_total).reshape(n_partial_core,1)\n",
    "        print('CPU map a : {0}'.format(cpu_map_a))\n",
    "        print('CPU map b : {0}'.format(cpu_map_b))\n",
    "        print(indices[cpu_map_b])\n",
    "\n",
    "        #############################################################################\n",
    "        #packages of data to be passed to an unwrapper to unwrap for multiprocessing#\n",
    "        #############################################################################\n",
    "\n",
    "        package_a = [(nchunks_total,\n",
    "                      cpu_map_a[i],\n",
    "                      indices,\n",
    "                      simPath,\n",
    "                      partIDs_loc,\n",
    "                      basePath,\n",
    "                      snapshot_number,\n",
    "                      offsetFile,\n",
    "                      subdir_name) for i in range(cpus_to_use)]\n",
    "        package_b = [(nchunks_total,\n",
    "                      cpu_map_b[i],\n",
    "                      indices,\n",
    "                      simPath,\n",
    "                      partIDs_loc,\n",
    "                      basePath,\n",
    "                      snapshot_number,\n",
    "                      offsetFile,\n",
    "                      subdir_name) for i in range(n_partial_core)]\n",
    "        #print('package a: {0}'.format(package_a))\n",
    "        #print('package b: {0}'.format(package_b))\n",
    "\n",
    "        #########################\n",
    "        #the multiprocessing bit#\n",
    "        #########################\n",
    "\n",
    "        print('full core')\n",
    "        with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package_a,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #even more terminating\n",
    "\n",
    "        print('partial core')\n",
    "        with closing(Pool(n_partial_core)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package_b,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #even more terminating\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98acedc",
   "metadata": {},
   "source": [
    "# Test to see if the chunked, parallelised output is the same as the original\n",
    "Note: will only work for smaller files e.g. TNG50-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed64fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose snapshot\n",
    "snapshot_to_test = 21\n",
    "#choose simulation\n",
    "sim_to_test = 'TNG50-4'\n",
    "\n",
    "#Load unchunked version of a dataset from home area\n",
    "Test_a = np.load('/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/Matchlist_dir_{0}/ShIDList_Snap{1}.npy'.format(sim_to_test,\n",
    "                                                                                                                  snapshot_to_test))\n",
    "\n",
    "#Load a chunked version of the same dataset and assemble from /ptmp/\n",
    "chunkedfiles = os.listdir('/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/'.format(sim_to_test))\n",
    "chunkedfiles = ['/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/{1}'.format(sim_to_test,i) for i in chunkedfiles]\n",
    "chunkedfiles = sorted([i for i in chunkedfiles if 'ShIDList_Snap{0}'.format(snapshot_to_test) in i])\n",
    "data = [np.load(i) for i in chunkedfiles]\n",
    "Test_b = np.concatenate(data)\n",
    "\n",
    "#print tests to see if they are the same\n",
    "print(Test_a,Test_b)\n",
    "print(np.unique(Test_a),np.unique(Test_b))\n",
    "print(Test_a.shape,Test_b.shape)\n",
    "print(Test_a==Test_b)\n",
    "print('Test to see if arrays are equal: {0}'.format(np.array_equal(Test_a,Test_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7602d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose snapshot\n",
    "snapshot_to_test = 99\n",
    "#choose simulation\n",
    "sim_to_test = 'TNG300-1'\n",
    "\n",
    "#Load a chunked version of the same dataset and assemble from /ptmp/\n",
    "chunkedfiles = os.listdir('/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/'.format(sim_to_test))\n",
    "chunkedfiles = ['/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/Matchlist_dir_{0}/{1}'.format(sim_to_test,i) for i in chunkedfiles]\n",
    "chunkedfiles = sorted([i for i in chunkedfiles if 'ShIDList_Snap{0}'.format(snapshot_to_test) in i])\n",
    "#data = [np.load(i) for i in chunkedfiles]\n",
    "print(len(chunkedfiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ########################################\n",
    "        #loop and load the particles to process#\n",
    "        ########################################\n",
    "        \n",
    "        for chunk_id in range(nchunks_total):\n",
    "            print('loading chunk {0}'.format(chunk_id))\n",
    "            chunk_first_idx = indices[chunk_id][0]\n",
    "            chunk_last_idx = indices[chunk_id][-1]\n",
    "            print('chunk indices: ({0},{1})'.format(chunk_first_idx,chunk_last_idx))\n",
    "\n",
    "            allparts = np.copy(f[partIDs_loc][chunk_first_idx:chunk_last_idx]) #load the particles in this chunk to ID\n",
    "            nparts = allparts.shape[0] #get number of particles in this chunk\n",
    "            print('chunk loaded.')\n",
    "\n",
    "            #create a list version of every particle ID in this chunk\n",
    "            AllPartList = allparts.tolist()\n",
    "       \n",
    "       \n",
    "            ########################\n",
    "            #CREATE SUBHALO ID LIST#\n",
    "            ########################\n",
    "   \n",
    "\n",
    "            #choose which particles to ID\n",
    "            to_ID = np.arange(chunk_first_idx,chunk_last_idx+1) #replace with the indices we want to load    \n",
    "    \n",
    "            partType = 0 #gas\n",
    "    \n",
    "            #choose subhalo fields to load\n",
    "            subhaloFields = ['SubhaloFlag',\n",
    "                             'SubhaloPos',\n",
    "                             'SubhaloHalfmassRad',\n",
    "                             'SubhaloHalfmassRadType',\n",
    "                             'SubhaloLenType']\n",
    "    \n",
    "            #load subhalo catalog\n",
    "            subhalos=il.groupcat.loadSubhalos(basePath,snapshot_number,fields=subhaloFields) \n",
    "            subhalos.keys()\n",
    "    \n",
    "    \n",
    "            #get subhalo offset file for matching particle and subhalo IDs\n",
    "            with h5py.File(offsetFile,'r') as f2:\n",
    "                SnapOffsetsSubhalo= np.copy(f2['/Subhalo/SnapByType'])\n",
    "    \n",
    "            #get subhalo lengths for all gas particles\n",
    "            SubhaloLenType = np.copy(subhalos['SubhaloLenType'])\n",
    "    \n",
    "            #create array of subhaloIDs for every gas particle\n",
    "            AllShIDList = pID2shID(to_ID,partType,SubhaloLenType,SnapOffsetsSubhalo)\n",
    "    \n",
    "            #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend the loading\n",
    "            #in the pipe creation code to parse through each part separately.\n",
    "            uniquestest_chunked.append(np.unique(AllShIDList).tolist())\n",
    "\n",
    "            np.save('/{0}/PartList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllPartList)\n",
    "            np.save('/{0}/ShIDList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllShIDList)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "143*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c201a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
