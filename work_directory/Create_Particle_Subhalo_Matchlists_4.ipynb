{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b261e42",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2d12f",
   "metadata": {},
   "source": [
    "This notebook creates subhalo ID and particle ID lists for snapshots for IllustrisTNG simulations.\n",
    "\n",
    "These can be loaded and used for cross-matching for impact factors to galaxies.\n",
    "\n",
    "The code is updated from: \n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/Create_Particle_Subhalo_Matchlists_3.ipynb\n",
    "\n",
    "using:\n",
    "\n",
    "raven:/u/cwalker/Illustris_Zhang_Method/test_artale_vs_yt_field_load.ipynb\n",
    "\n",
    "- It builds on previous work by creating lists for any desired simulation.\n",
    "- It uses illustris_python instead of yt to load simulation particle ID list.\n",
    "- It loads particle ID data and creates subhalo IDs in chunks, and stores the matchlists in chunks to accomodate larger simulations (e.g. TNG300-1)\n",
    "- As output for larger simulations is huge, it now stores matchlists to /ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/\n",
    "- It is the version of the code which is parallelised and tested in prior notebooks.\n",
    "\n",
    "#NOTES\n",
    "\n",
    "- NOTE 10/03/22: in the code, I feed to_ID into pID2shID(). I think this is correct.\n",
    "- #this would mean I am feeding it the INDICES of the particle IDs, not the particle IDs themselves.\n",
    "- #this would mean I need to update the documentation of this function.\n",
    "- #this is tested below by feeding it AllPartList instead of to_ID, which always returns -1.\n",
    "    \n",
    "- #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend \n",
    "- #the loading in the pipe creation code to parse through each part separately.\n",
    "    \n",
    "- #NOTE: If you run both the chunked and non-chunked versions of this code for a single snapshot\n",
    "- #for a small simulation, you can perform the test and see the output is the same.\n",
    "    \n",
    "- #NOTE 17/03/22: sometimes if you use too many cores the multiprocessing version hangs after ccompletion.\n",
    "- #Don't know how to fix this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb572e15",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d500c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raven/u/cwalker/virtual_environments/yt-git/yt/utilities/logger.py:4: VisibleDeprecationWarning: The configuration file /u/cwalker/.config/yt/ytrc is deprecated in favor of /u/cwalker/.config/yt/yt.toml. Currently, both are present. Please manually remove the deprecated one to silence this warning.\n",
      "Deprecated since v4.0.0 . This feature will be removed in v4.1.0\n",
      "  from yt.config import ytcfg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yt\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing as m\n",
    "import illustris_python as il\n",
    "\n",
    "from contextlib import closing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from charlie_TNG_tools import pID2shID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81992cec",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_matchlist_package(package):\n",
    "    \"\"\"\n",
    "    Helper function for making simulation matchlists in parallel using multiprocessing.\n",
    "    Unpacks the set of data necessary for parsing the simulation and making matchlists\n",
    "    for certain chunks. Then makes those chunks with process_matchlist_chunk().\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    package : a list containing the inpyt data, which are X arguments in the following order:\n",
    "    \n",
    "        nchunks_total   : [int] the total number of chunks the simulation will be split into to create\n",
    "                        matchlist covering the entire simulation\n",
    "        chunkIDs        : [array of ints] the id numbers of the chunks of simulation to be processed\n",
    "                        on this cpu\n",
    "        indices         : [array] array of particle indices to load for each whole chunk. shape of \n",
    "                        (nchunks_total,2)\n",
    "        simPath         : [str] location of sim file to be loaded as hdf5 File containing all the particle IDs\n",
    "                        subhalo IDs will be created for\n",
    "        partIDs_loc     : [str] location of the (partType0) particle IDs for this simulation snapshot\n",
    "        basePath        : [str] base path to the simulation data\n",
    "        snapshot_number : [int] the snapshot of the simulation to process\n",
    "        offsetFile      : [str] location of the offset file for this simulation and snapshot\n",
    "        subdir_name     : [str] location of subdirectory to store output matchlist chunk in\n",
    "    \n",
    "    RETURNS:\n",
    "    \n",
    "    output of process_matchlist_chunk()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    verbose=True\n",
    "    \n",
    "    #unwrap the package to feed to process_matchlist_chunk()\n",
    "    nchunks_total   = package[0]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: nchunks_total = {0}'.format(nchunks_total))\n",
    "    chunkIDs        = package[1]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: chunkIDs = {0}'.format(chunkIDs))\n",
    "    indices         = package[2]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: indices = {0}'.format(indices))\n",
    "    simPath         = package[3]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: simPath = {0}'.format(simPath))\n",
    "    partIDs_loc     = package[4]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: partIDs_loc = {0}'.format(partIDs_loc))\n",
    "    basePath        = package[5]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: basePath = {0}'.format(basePath))\n",
    "    snapshot_number = package[6]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: snapshot_number = {0}'.format(snapshot_number))\n",
    "    offsetFile      = package[7]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: offsetFile = {0}'.format(offsetFile))\n",
    "    subdir_name     = package[8]\n",
    "    if verbose==True:\n",
    "        print('Verbose mode check: subdir_name = {0}'.format(subdir_name))\n",
    "    \n",
    "    #run process_matchlist_chunk()\n",
    "    process_matchlist_chunk(nchunks_total,chunkIDs,indices,simPath,partIDs_loc,basePath,snapshot_number,offsetFile,subdir_name)\n",
    "    \n",
    "    return 'done'\n",
    "\n",
    "def process_matchlist_chunk(nchunks_total,\n",
    "                            chunkIDs,\n",
    "                            indices,\n",
    "                            simPath,\n",
    "                            partIDs_loc,\n",
    "                            basePath,\n",
    "                            snapshot_number,\n",
    "                            offsetFile,\n",
    "                            subdir_name):\n",
    "    \"\"\"\n",
    "\n",
    "    For a simulation snapshot split into nchunks chunks,  creates certain chunks of the simulation \n",
    "    particle/subhalo ID matchlist on a single cpu. It is fed by unwrap_matchlist_package().\n",
    "    \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    \n",
    "    nchunks_total   : [int] the total number of chunks the simulation will be split into to create\n",
    "                    matchlist covering the entire simulation\n",
    "    chunkIDs        : [array of ints] the id numbers of the chunks of simulation to be processed\n",
    "                    on this cpu\n",
    "    indices         : [array] array of particle indices to load for each whole chunk. shape of \n",
    "                    (nchunks_total,2)\n",
    "    simPath         : [str] location of sim file to be loaded as hdf5 File containing all the particle IDs  \n",
    "                    subhalo IDs will be created for\n",
    "    partIDs_loc     : [str] location of the (partType0) particle IDs for this simulation snapshot\n",
    "    basePath        : [str] base path to the simulation data\n",
    "    snapshot_number : [int] the snapshot of the simulation to process\n",
    "    offsetFile      : [str] location of the offset file for this simulation and snapshot\n",
    "    subdir_name     : [str] location of subdirectory to store output matchlist chunk in\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    1 (must return something so multiprocessing is guaranteed to terminate after process is finished)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #print(simPath)\n",
    "\n",
    "    #get all gas particle IDs in the simulation snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "\n",
    "        print(f)\n",
    "        \n",
    "        #for the chunks of the simulation to be processed by this CPU...\n",
    "        for i in range(len(chunkIDs)):\n",
    "            \n",
    "            ########################################\n",
    "            #CREATE THE CHUNK PARTICLE ID MATCHLIST#\n",
    "            ########################################\n",
    "            \n",
    "            #select the chunk ID to be processed\n",
    "            chunk_id = chunkIDs[i] \n",
    "\n",
    "            #extract the indices within this chunk which will be loaded\n",
    "            print('loading chunk {0}'.format(chunk_id))\n",
    "            chunk_first_idx = indices[chunk_id][0]\n",
    "            chunk_last_idx = indices[chunk_id][-1]\n",
    "            print('chunk indices: ({0},{1})'.format(chunk_first_idx,chunk_last_idx))\n",
    "\n",
    "            #load only the particles in this chunk, according to the indices\n",
    "            allparts = np.copy(f[partIDs_loc][chunk_first_idx:chunk_last_idx])\n",
    "\n",
    "            #get number of particles in this chunk\n",
    "            nparts = allparts.shape[0] \n",
    "            print('chunk loaded.')\n",
    "\n",
    "            #create a list version of every particle ID in this chunk\n",
    "            AllPartList = allparts.tolist()\n",
    "            print('Particle list created')\n",
    "\n",
    "            #######################################\n",
    "            #CREATE THE CHUNK SUBHALO ID MATCHLIST#\n",
    "            #######################################\n",
    "\n",
    "            #choose which particles to ID\n",
    "            to_ID = np.arange(chunk_first_idx,chunk_last_idx+1) #replace with the indices we want to load    \n",
    "\n",
    "            #choose the particle type (note: 0 is gas, and it should always be gas, but perhaps this)\n",
    "            #should not be hard coded...\n",
    "            partType = 0 #gas\n",
    "\n",
    "            #choose subhalo fields to load\n",
    "            subhaloFields = ['SubhaloFlag',\n",
    "                             'SubhaloPos',\n",
    "                             'SubhaloHalfmassRad',\n",
    "                             'SubhaloHalfmassRadType',\n",
    "                             'SubhaloLenType']\n",
    "\n",
    "            #load subhalo catalog\n",
    "            subhalos=il.groupcat.loadSubhalos(basePath,snapshot_number,fields=subhaloFields) \n",
    "            subhalos.keys()\n",
    "\n",
    "            #get subhalo offset file for matching particle and subhalo IDs\n",
    "            with h5py.File(offsetFile,'r') as f2:\n",
    "                SnapOffsetsSubhalo= np.copy(f2['/Subhalo/SnapByType'])\n",
    "            print('offsetfile loaded')\n",
    "\n",
    "            #get subhalo lengths for all gas particles\n",
    "            SubhaloLenType = np.copy(subhalos['SubhaloLenType'])\n",
    "\n",
    "            #create array of subhaloIDs for every gas particle\n",
    "            AllShIDList = pID2shID(to_ID,partType,SubhaloLenType,SnapOffsetsSubhalo)\n",
    "            print('ShIDList created')\n",
    "\n",
    "            #NOTE: you can run this for TNG300-1 in parts and save them in parts. you would then have to amend the loading\n",
    "            #in the pipe creation code to parse through each part separately.\n",
    "            #uniquestest_chunked.append(np.unique(AllShIDList).tolist())\n",
    "            \n",
    "            ############################\n",
    "            #SAVE BOTH MATCHLIST CHUNKS#\n",
    "            ############################\n",
    "            \n",
    "            np.save('/{0}/PartList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllPartList)\n",
    "            np.save('/{0}/ShIDList_Snap{1}_Chunk{2:04d}.npy'.format(subdir_name,snapshot_number,chunk_id),AllShIDList)  \n",
    "            print('saved')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0f4d9",
   "metadata": {},
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation to create lists for\n",
    "sim_to_use = 'TNG300-1'\n",
    "\n",
    "#snapshots to create lists for\n",
    "snap_list = [25]#[99,91,84,78,72,67,59,50,40,33,25,21,17]\n",
    "\n",
    "#base path to data\n",
    "basePath = '/virgo/simulations/IllustrisTNG/{0}/output/'.format(sim_to_use)\n",
    "\n",
    "#path to simulation hdf5 file\n",
    "simPath = '/virgo/simulations/IllustrisTNG/{0}/simulation.hdf5'.format(sim_to_use)\n",
    "\n",
    "#check these exist\n",
    "print('Testing whether basePath and simPath exist...')\n",
    "print('basePath exists = {0}'.format(os.path.exists(basePath)))\n",
    "print('simPath exists = {0},\\n{1}'.format(os.path.exists(simPath),simPath))\n",
    "\n",
    "nchunks=1000 #the desied number of chunks (minus remainder) to process the particle data in\n",
    "\n",
    "#identify number of available cores on the system\n",
    "ncpus = m.cpu_count()\n",
    "print('Maximum of {0} cores available to use.'.format(ncpus))\n",
    "\n",
    "cpus_to_use = 8\n",
    "print('Desired number of cores to use: {0}'.format(cpus_to_use))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4e6ed",
   "metadata": {},
   "source": [
    "# Create directories to store matchlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see if a top directory exists\n",
    "#topdir_name = '/u/cwalker/Illustris_Zhang_Method/Sim_Matchlists/'\n",
    "topdir_name = '/ptmp/cwalker/Illustris_FRB_Project/Sim_Matchlists/' #edit 14/03/2022: put in /ptmp/ now due to size for large simulations\n",
    "topdir_check = os.path.exists(topdir_name)\n",
    "\n",
    "#if it exists, print\n",
    "if topdir_check == True:\n",
    "    print('Top directory ({0}) already exists.'.format(topdir_name))\n",
    "    \n",
    "#else, create it.\n",
    "elif topdir_check == False:\n",
    "    print('Creating top directory for matchlists at {0}...'.format(topdir_name))\n",
    "    os.mkdir(topdir_name)\n",
    "    print('{0} created.'.format(topdir_name))\n",
    "    \n",
    "    \n",
    "#check to see if subdirectory for particular simulation matchlist exists\n",
    "subdir_name = topdir_name+'Matchlist_dir_{0}'.format(sim_to_use)\n",
    "subdir_check = os.path.exists(subdir_name)\n",
    "\n",
    "#if it exists, print\n",
    "if subdir_check == True:\n",
    "    print('Directory to hold {0} matchlist ({1} exists.)'.format(sim_to_use,subdir_name))\n",
    "\n",
    "#else, create it\n",
    "elif subdir_check == False:\n",
    "    print('Creating subdirectory {0}...'.format(subdir_name))\n",
    "    os.mkdir(subdir_name)\n",
    "    print('{0} created.'.format(subdir_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508a539",
   "metadata": {},
   "source": [
    "# Create matchlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4826013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop over snapshots\n",
    "for snapshot_number in snap_list:\n",
    "    \n",
    "    uniquestest_chunked = [] #test array to record all unique subhalo IDs to test against the non-chunked version\n",
    "    \n",
    "    print('Creating for {0} snap {1}'.format(sim_to_use,snapshot_number))\n",
    "    \n",
    "     #########################\n",
    "     #SNAPSHOT DATA LOCATIONS#\n",
    "     #########################\n",
    "\n",
    "    offsetFile = '/virgo/simulations/IllustrisTNG/{0}/postprocessing/offsets/offsets_0{1}.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    data_loc = '/virgo/simulations/IllustrisTNG/{0}/output/snapdir_0{1}/snap_0{1}.0.hdf5'.format(sim_to_use,snapshot_number)\n",
    "    partIDs_loc = '/Snapshots/{0}/PartType0/ParticleIDs'.format(snapshot_number)\n",
    "    \n",
    "    print('Processing snapshot {0} at:\\n{1}\\n with offset file:\\n{2}\\n and particle IDs file loc:\\n{3}'.format(snapshot_number,data_loc,offsetFile,partIDs_loc))\n",
    "\n",
    "    \n",
    "    #########################\n",
    "    #CREATE PARTICLE ID LIST#\n",
    "    #########################\n",
    "    \n",
    "    \n",
    "    #get all gas particle IDs in snapshot\n",
    "    with h5py.File(simPath) as f:\n",
    "        \n",
    "        \n",
    "        #you can load smaller chunks of this at a time! Exploit this.\n",
    "        \n",
    "        \n",
    "        simparts = f[partIDs_loc].shape[0] #get the total number of particles in the simulation\n",
    "        \n",
    "        ########################\n",
    "        ########################\n",
    "        ##CHUNKING INFORMATION##\n",
    "        ########################\n",
    "        ########################\n",
    "        \n",
    "        #######################################################################################\n",
    "        #the number of chunks of data to load the particle IDs in according to desired nchunks#\n",
    "        #######################################################################################\n",
    "        \n",
    "        print('calculating chunking information for {0} particles over {1} desired chunks...'.format(simparts,\n",
    "                                                                                                     nchunks))\n",
    "        \n",
    "        nchunks_whole = nchunks #number of whole chunks to process\n",
    "        parts_per_chunk = simparts//nchunks #number of particles in a whole chunk which will be processed\n",
    "        remainder = simparts%nchunks #number of particles in the remainder once whole chunks are processed.\n",
    "        if remainder!=0: #if there is any remainder...\n",
    "            nchunks_partial = 1 #one extra chunk will be processed, which is not the same size as the others\n",
    "        else: #if there is no remainder...\n",
    "            nchunks_partial = 0 #no extra chunks will be processed.\n",
    "        nchunks_total = nchunks_whole + nchunks_partial #the total number of chunks (plus any remainder) to process\n",
    "        \n",
    "        print('will process simulation in {0} chunks. {1} chunks of {2} particles, and {3} chunks of {4}.'.format(nchunks_total,\n",
    "                                                                                                                 nchunks_whole,\n",
    "                                                                                                                  parts_per_chunk,\n",
    "                                                                                                                  nchunks_partial,\n",
    "                                                                                                                  remainder))\n",
    "        ###############################################################################                                                                                                    \n",
    "        #how this loading will be distributed over desired number of cores cpus_to_use#  \n",
    "        ###############################################################################\n",
    "\n",
    "        #choose the number of cores to use at once. \n",
    "        cpus_to_use = cpus_to_use \n",
    "\n",
    "        #calculate the number of full core runs to be used to check for simulation cells in pipe\n",
    "        #this number is the number of parts of the simulation which will be loaded simultaneously\n",
    "        n_full_core = nchunks_total//cpus_to_use\n",
    "        print(n_full_core)\n",
    "        \n",
    "\n",
    "        #calculate the number of cores which must be used to check the remaining simulation cells\n",
    "        #this number is the number of leftover parts of the simulation which will be loaded all at once\n",
    "        n_partial_core = nchunks_total%cpus_to_use\n",
    "\n",
    "        print('To parse these {0} chunks over {1} cpus, {1} cpus will load data simultaneously.\\n\\\n",
    "              This will happen {2} times. The remaining data needs {3} cpus.\\n\\\n",
    "              These will be loaded simultaneously.'.format(nchunks_total,\n",
    "                                                           cpus_to_use,\n",
    "                                                           n_full_core,\n",
    "                                                           n_partial_core))\n",
    "        \n",
    "        ###################################\n",
    "        #indices of chunks (and remainder)#\n",
    "        ###################################\n",
    "        \n",
    "        #calculate the particle indices to load for each whole chunk\n",
    "        indices = [[i*parts_per_chunk,(i*parts_per_chunk)+(parts_per_chunk-1)] for i in range(nchunks_whole)]\n",
    "        #append the particle indices for the remainder chunk\n",
    "        indices.append([nchunks*(parts_per_chunk),(nchunks*parts_per_chunk)-1+(remainder)])\n",
    "        indices=np.array(indices)\n",
    "        print('particles will be loaded according to indices array of shape {0}:\\n\\\n",
    "        {1}'.format(indices.shape,\n",
    "                    indices))\n",
    "        \n",
    "    ###################                   \n",
    "    ###################                                                                                                          \n",
    "    ##PROCESSING DATA##                                                                                                          \n",
    "    ###################\n",
    "    ###################\n",
    "\n",
    "    if n_partial_core == 0: #if there is no remainder to process after full core runs\n",
    "\n",
    "        print('No partial cores to process')\n",
    "\n",
    "        ####################################################\n",
    "        #map of which indices will be processed on each cpu#\n",
    "        ####################################################\n",
    "\n",
    "        cpu_map = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core) \n",
    "        print('CPU map: {0}'.format(cpu_map))\n",
    "\n",
    "        ############################################################################\n",
    "        #package of data to be passed to an unwrapper to unwrap for multiprocessing#\n",
    "        ############################################################################\n",
    "\n",
    "        package = [(nchunks_total,\n",
    "                    cpu_map[i],\n",
    "                    indices,\n",
    "                    simPath,\n",
    "                    partIDs_loc,\n",
    "                    basePath,\n",
    "                    snapshot_number,\n",
    "                    offsetFile,\n",
    "                    subdir_name) for i in range(cpus_to_use)]\n",
    "        #print('package: {0}'.format(package))\n",
    "\n",
    "        #########################\n",
    "        #the multiprocessing bit#\n",
    "        #########################\n",
    "\n",
    "        with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #eeven more terminating\n",
    "\n",
    "    elif n_partial_core > 0: #else if there is remainder to process after full core runs\n",
    "\n",
    "        print('Partial cores to process')\n",
    "\n",
    "        #####################################################\n",
    "        #maps of which indices will be processed on each cpu#\n",
    "        #####################################################\n",
    "\n",
    "        cpu_map_a = np.arange(n_full_core*cpus_to_use).reshape(cpus_to_use,n_full_core)\n",
    "        cpu_map_b = np.arange(n_full_core*cpus_to_use,nchunks_total).reshape(n_partial_core,1)\n",
    "        print('CPU map a : {0}'.format(cpu_map_a))\n",
    "        print('CPU map b : {0}'.format(cpu_map_b))\n",
    "        print(indices[cpu_map_b])\n",
    "\n",
    "        #############################################################################\n",
    "        #packages of data to be passed to an unwrapper to unwrap for multiprocessing#\n",
    "        #############################################################################\n",
    "\n",
    "        package_a = [(nchunks_total,\n",
    "                      cpu_map_a[i],\n",
    "                      indices,\n",
    "                      simPath,\n",
    "                      partIDs_loc,\n",
    "                      basePath,\n",
    "                      snapshot_number,\n",
    "                      offsetFile,\n",
    "                      subdir_name) for i in range(cpus_to_use)]\n",
    "        package_b = [(nchunks_total,\n",
    "                      cpu_map_b[i],\n",
    "                      indices,\n",
    "                      simPath,\n",
    "                      partIDs_loc,\n",
    "                      basePath,\n",
    "                      snapshot_number,\n",
    "                      offsetFile,\n",
    "                      subdir_name) for i in range(n_partial_core)]\n",
    "        #print('package a: {0}'.format(package_a))\n",
    "        #print('package b: {0}'.format(package_b))\n",
    "\n",
    "        #########################\n",
    "        #the multiprocessing bit#\n",
    "        #########################\n",
    "\n",
    "        print('full core')\n",
    "        with closing(Pool(cpus_to_use)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package_a,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #even more terminating\n",
    "\n",
    "        print('partial core')\n",
    "        with closing(Pool(n_partial_core)) as p: #invoke multiproccessing\n",
    "            run = p.map(unwrap_matchlist_package,package_b,chunksize=1) #run the multiprocessing\n",
    "        p.terminate() #terminate after completion\n",
    "        p.join() #even more terminating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d461207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
